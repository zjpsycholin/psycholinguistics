+++
title = "Week 7: 🧩 Sentence Processing I"
date = "2025-05-24T10:03:44-04:00"
draft = false
+++

To be published ...




<!--
## 📘 Overview
When we read or listen, we **don’t** wait for a sentence to end—we build structure **incrementally**, word by word. This can lead to **temporary ambiguity** and “garden-path” experiences where our first guess turns out wrong and we must **reanalyse**. This week introduces **core parsing strategies** (e.g., **Minimal Attachment**, **Late Closure**), classic garden-path examples, and the basic contrast between **syntax-first** and **constraint-based** views of comprehension.

---

## 🎯 Learning Goals
By the end of Week 7, you should be able to:

- Explain **incremental parsing** and why temporary ambiguity is common.
- Identify and diagnose **garden-path sentences** and describe **reanalysis**.
- Describe **Minimal Attachment** and **Late Closure** and apply them to examples.
- Contrast **syntax-first** vs **constraint-based/interactive** accounts at a basic level.
- Use **plausibility** and **prosody** intuitions to predict easier vs harder readings.

---

## 📖 Required Reading
- **Traxler (1st ed.), Chapter 4, pp. 141–154** — *Sentence Processing* (introduction, ambiguity, garden paths, core strategies).

---

## 🔑 Key Concepts & Mini-Explanations

### ⏱️ Incremental Parsing & Temporary Ambiguity
- **Incremental**: The parser commits to a structure as each word arrives.
- **Temporary ambiguity**: Early words allow **multiple** structures. Later words can **disconfirm** the initial choice → **garden-path**.

### 🌿 Garden-Path Effect & Reanalysis
- **Garden-path sentence**: lures the parser into a wrong analysis, forcing a **backtrack**.
  - *Example*: “While Anna dressed **the baby** played in the crib.”  
    Initial parse treats **the baby** as object of **dressed**; later **played** forces reanalysis (the baby = subject of played).
- **Reanalysis cost**: measurable slowdowns (longer reading times, regressions in eye-tracking).

### 🧭 Two Heuristics: Minimal Attachment & Late Closure
- **Minimal Attachment (MA)**: Prefer the structure with **fewer new nodes** (simplest tree).  
- **Late Closure (LC)**: Attach incoming material to the **phrase currently being processed** (keep it in the current clause/VP) whenever grammatically possible.
- These heuristics predict classic garden-paths such as reduced relatives:
  - *“The horse **raced past the barn** fell.”*  
    MA/LC bias “raced” as main-verb; later “fell” forces reanalysis to **reduced relative**: *The horse [that was raced past the barn] fell.*

### 🧱 Syntax-First vs Constraint-Based (Big Picture)
- **Syntax-first**: An initial, **purely structural** parse (driven by MA/LC); semantics/pragmatics integrate **after** a first pass.
- **Constraint-based (interactive)**: Multiple interpretations compete; **syntax, semantics, frequency, plausibility, prosody** all contribute **from the start**. Strong context can **prevent** some garden-paths.

### 🧪 Cues that Modulate Difficulty
- **Verb bias/subcategorization** (expectation for certain complements).  
- **Plausibility** (world knowledge can discourage unlikely parses).  
- **Prosody/Punctuation** (comma intonation can reduce garden-paths in speech/writing).  
- **Morphology/Function words** (that/which, case markers, aspect cues) guide attachment decisions.

---

## 📝 Pre-Class Activities
1. **Read** pp. 141–154 and flag any sentence that made you **slow down**.  
2. **Diagnose 3 sentences**: For each, write your **initial parse** and the **final correct parse** (one can be from the textbook).  
3. **Prosody experiment**: Read aloud “While the man hunted the deer ran into the woods” twice—once with a pause after **hunted**. Which felt clearer?

---

## 💬 In-Class Activities

### 1) Garden-Path Game (12 min)
- **Individually** read a list of 8 sentences. Mark **where** you felt confusion.  
- **Pairs**: decide **why** (MA? LC? Verb bias?).  
- Share 2 cases with the class.

**Sample set**
1. While the man hunted the deer ran into the woods.  
2. The old train the young.  
3. The horse raced past the barn fell.  
4. When Fred eats food gets thrown.  
5. Since Jay always jogs a mile seems easy.  
6. The raft floated down the river sank.

### 2) Strategy Lab: MA vs LC (15 min)
- **Triads**: Label each attachment decision (**Minimal Attachment** or **Late Closure**).  
- Rewrite 2 sentences to **disarm** the garden-path using commas, **that**, or verb changes.  
  - e.g., *While the man hunted, the deer ran…* / *The horse that was raced…*

### 3) Plausibility vs Structure (12 min)
- Consider pairs where structure is similar but **plausibility** differs:  
  - a) *The evidence examined by the lawyer was compelling.*  
  - b) *The prisoner examined by the lawyer was compelling.*  
- **Discuss**: Which one invites a main-verb parse for **examined**? Why does world knowledge matter?

### 4) Mini Prosody Workshop (8 min)
- Read ambiguous items **with** and **without** a pause/comma.  
- **Note**: Did a prosodic boundary help you pick the intended structure?

### 5) Quick Wrap (3 min)
- On a sticky: write **one heuristic** (MA/LC) and **one place** it can mislead.

---

## 🔁 Post-Class Review
- **One-pager**: For **two** garden-paths, chart: initial parse → disconfirming word → correct parse → which heuristic misled you.  
- **Reflection (100–120 words)**: Do your L1 (Chinese) cues (particles, word order, aspect markers) help you **avoid** certain English garden-paths?

---

## 🏠 Homework
- **Textbook “Test Yourself”** from pp. 141–154 (items on ambiguity and garden-paths).  
- **Short write-up (≈150–200 words)**: Compare **syntax-first** vs **constraint-based** using **one** sentence from class; predict how **context** or **prosody** would change the difficulty.

---

## 🧩 Self-Check Questions

**Q1.** What is a **garden-path sentence** and why does it occur?  
<!-- A garden-paths lures the parser into an initially plausible but wrong structure during incremental parsing; later input forces reanalysis, causing slowdown. -->
<!--
**Q2.** Define **Minimal Attachment** and **Late Closure** in one sentence each.  -->
<!-- Minimal Attachment: prefer the parse with the fewest new nodes (simplest structure). Late Closure: attach new material to the phrase currently being processed whenever grammar allows. -->
<!--
**Q3.** Give one example where **prosody** reduces a garden-path.  -->
<!-- Adding a pause/comma: “While the man hunted, the deer ran into the woods.” The boundary discourages attaching "the deer" as the object of "hunted." -->
<!--
**Q4.** How would a **syntax-first** account differ from a **constraint-based** account for “The horse raced past the barn fell”?  -->
<!-- Syntax-first: parser initially builds a main-clause parse (MA/LC), then reanalyses to a reduced relative when "fell" arrives. Constraint-based: with enough cues (frequency, plausibility, prosody), the reduced-relative parse can be activated earlier, reducing garden-pathing. -->
<!--
**Q5.** Why do **verb biases** matter for ambiguity?  -->
<!-- Verbs differ in preferred complements; if a verb strongly prefers a direct object or a clause, the parser's expectations shift, affecting early attachment choices and garden-path likelihood. -->

---
<!--
## 🧰 Key Terms
**Incremental parsing**, **Temporary ambiguity**, **Garden-path**, **Reanalysis**, **Minimal Attachment**, **Late Closure**, **Reduced relative**, **Verb bias**, **Syntax-first**, **Constraint-based/interactive**, **Prosody**.

---

## 🌐 Optional Resources
- Short explainers/demos of **garden-path sentences** and parsing heuristics.  
- Audio examples showing **prosodic disambiguation** (pause/comma intonation).  
- Beginner blog posts on **why “The horse raced past the barn fell” is hard**.

---

### ✅ How to use these notes
- **Before class:** preview examples; try reading them aloud with/without pauses.  
- **During class:** name the **heuristic** behind your first analysis.  
- **After class:** practice rewriting garden-paths with **disambiguating cues** (that, commas, different verbs).


-->









<!--
## 📘 Overview

This week focuses on **discourse-level comprehension**: how we go from understanding isolated sentences to constructing **integrated mental models** of entire narratives or conversations. You'll learn three influential models—**Construction-Integration**, **Structure Building**, and **Event Indexing**—and examine how knowledge, memory, and coherence contribute to understanding stories and discourse.

---

## 🧠 Core Concepts

### What Is Discourse Processing?

- Involves understanding **connected text** or conversation, not just isolated sentences.
- Requires building a **situation model**: a mental simulation of the events described.

---

### Model 1: Construction–Integration Theory

| Stage | Description |
|-------|-------------|
| **Construction** | Generate all possible meanings (including irrelevant or contradictory ones). |
| **Integration** | Use context and prior knowledge to suppress irrelevant meanings and **integrate** consistent ideas. |

- Influenced by **spreading activation** and **network models**.
- Highlights importance of **text coherence** and **prior knowledge** in reducing ambiguity.

---

### Model 2: Structure Building Framework

| Component | Role |
|----------|------|
| **Laying a Foundation** | Readers begin building a representation when they encounter a new topic or idea. |
| **Mapping** | New info is integrated if related to the current structure. |
| **Shifting** | If unrelated, a **new substructure** is created (topic shift, tense change, etc.).

- **Enhancement** strengthens relevant nodes; **suppression** reduces activation of unrelated ideas:contentReference[oaicite:0]{index=0}.

---

### Model 3: Event Indexing Model

- Readers track **5 key dimensions** to understand a situation:

| Dimension | Example |
|-----------|---------|
| **Time** | Do events occur at the same time? |
| **Space** | Are characters in the same location? |
| **Entity** | Are the same people or objects involved? |
| **Causality** | Are events causally linked? |
| **Intentionality** | Are goals or motivations shared? |

- **Similarity across dimensions** helps maintain coherence and facilitates comprehension:contentReference[oaicite:1]{index=1}.

---

### Causal and Coherence Inference

- We use **background knowledge** and **semantic cues** to make inferences:
  - Temporal ordering
  - Cause-effect relationships
  - Goal fulfillment
- **Bridging inferences**: connect sentences across gaps.
- **Elaborative inferences**: enrich the model with relevant but unstated info.

---

## 📚 Reading

- Traxler (2012), Chapter 5: *Discourse Processing* (pp. 187–210)

---

## 🏷️ Key Terms

| Term | Definition |
|------|------------|
| **Discourse** | Connected sequences of language (text or talk) |
| **Situation Model** | Mental representation of what a discourse describes |
| **Construction-Integration Model** | Framework describing how meanings are initially constructed and then refined |
| **Structure Building** | A model of comprehension involving foundation, mapping, and shifting |
| **Event Indexing** | Model focusing on dimensions tracked during discourse understanding |

---

## 🧪 Examples & In-Class Activities

### 🔁 Coherence Building Challenge

- Present scrambled story events (e.g., “She cried. He left. They fought.”).
- Students reorder and justify the most **coherent** narrative.

### 📚 Story Retelling

- Read a short paragraph aloud.
- In pairs, students retell the story to a peer from memory.
- Discuss what parts were emphasized, omitted, or inferred.

### 🧩 Event Indexing Game

- Provide mini-stories that vary one event dimension (e.g., space or causality).
- Ask students which version would be harder to follow and why.

---

## ❓ Self-Check Questions

1. What are the three major models of discourse processing introduced in this chapter?
2. How do readers use world knowledge to fill in gaps in discourse?
3. What is a situation model, and how is it constructed?
4. What dimensions are tracked according to the Event Indexing Model?
5. How does coherence affect memory and comprehension?

---

## 🧠 Practice Prompt (Adapted)

> Read the following two-sentence discourse:  
> “Jason dropped the vase. The floor was wet.”  
> - What kind of inference do you need to make to understand this connection?  
> - Which model(s) of discourse processing best explain this?

---

## 🔁 Related Chapters

- Chapter 4: *Sentence Processing* (input to discourse)
- Chapter 6: *Reference* (linking referents across discourse)
-->