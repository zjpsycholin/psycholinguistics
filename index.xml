<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Welcome on Introduction to Psycholinguistics</title><link>https://zjpsycholin.github.io/psycholinguistics/</link><description>Recent content in Welcome on Introduction to Psycholinguistics</description><generator>Hugo</generator><language>en-us</language><copyright>© Zhang Jun</copyright><lastBuildDate>Mon, 23 Jun 2025 10:03:44 -0400</lastBuildDate><atom:link href="https://zjpsycholin.github.io/psycholinguistics/index.xml" rel="self" type="application/rss+xml"/><item><title>Week 17: Reflections and Wrap-Up</title><link>https://zjpsycholin.github.io/psycholinguistics/week17/</link><pubDate>Mon, 23 Jun 2025 10:03:44 -0400</pubDate><guid>https://zjpsycholin.github.io/psycholinguistics/week17/</guid><description>&lt;p>To be published &amp;hellip;&lt;/p>
&lt;!--
## 🎯 Overview

As we approach the end of the course, this week offers time to reflect on what you've learned, how your thinking has evolved, and how psycholinguistics connects with your academic and personal interests. We’ll also discuss big-picture questions, review course themes, and gather your feedback to improve future iterations of this course.

---

## 📘 Core Activities

- Course-wide **wrap-up discussion**
- Submit your **final project write-up** (if assigned)
- Optional **personal reflection** or learning journal
- Participate in an **anonymous course feedback survey**

---

## 🧠 Recap and Integration

We'll revisit key ideas and ask:

- What were the most surprising or memorable insights from the course?
- How did your understanding of language and mind change?
- Which topics would you like to explore more deeply in the future?
- How do psycholinguistic principles apply to real-world communication?

---

## 📝 Final Project Write-Up (if applicable)

If your group or individual project includes a final written component, it is due this week.

**Guidelines (unless otherwise noted by instructor):**

- 800–1,500 words (flexible depending on format)
- Include:
 - Title
 - Research question/topic
 - Background and rationale
 - Method, analysis, or argument
 - Key findings or reflections
 - References (if used)

Submit via [platform or method specified by instructor].

---

## ✍️ Optional: Personal Reflection

You are encouraged (but not required) to write a brief reflection on your learning this semester.

Prompts may include:
- What challenged you most in this course?
- What are you most proud of?
- Has this course changed how you think about language or mind?
- What would you say to a future student considering this course?

---

## 📋 Anonymous Course Feedback

Please take 5–10 minutes to fill out the anonymous course feedback form.

Your responses are **confidential** and help improve future versions of this course.

[Insert feedback link here]

---

## 🎉 Congratulations!

You’ve reached the end of *Introduction to Psycholinguistics*. Whether you’re continuing in linguistics, psychology, education, or another field, we hope this course has helped you see language as a rich and fascinating window into the human mind.

--></description></item><item><title>Week 16: 🧠 Aphasia — The Breakdown of Language</title><link>https://zjpsycholin.github.io/psycholinguistics/week16/</link><pubDate>Wed, 18 Jun 2025 10:03:44 -0400</pubDate><guid>https://zjpsycholin.github.io/psycholinguistics/week16/</guid><description>&lt;p>To be published &amp;hellip;&lt;/p>
&lt;!--
## 📘 Overview
Aphasia is a **language** disorder caused by **brain damage** (typically stroke), affecting **speaking, understanding, reading,** and/or **writing**. In this week, we use classic syndromes—**Broca’s**, **Wernicke’s**, and **conduction aphasia**—to ask: *How is language organized in the brain?* We examine the traditional **Wernicke–Lichtheim–Geschwind (WLG)** model and why strict one-area→one-syndrome mappings often **break down**. You’ll analyze real **case profiles**, practice **lesion mapping**, and experience tasks that reveal **agrammatic comprehension**, **repetition failures**, and **naming deficits**.

---

## 🎯 Learning Goals
By the end of Week 16, you should be able to:

- Describe hallmark symptoms of **Broca’s**, **Wernicke’s**, and **conduction** aphasia (fluency, comprehension, repetition, naming).
- Explain the classical **WLG model** (Broca’s area, Wernicke’s area, arcuate fasciculus) and identify its **predictive limits**.
- Interpret why **lesion–symptom relationships** are often **distributed** rather than perfectly localized.
- Diagnose simplified **case vignettes** and justify your decisions with **behavioral evidence**.
- Predict comprehension difficulty for **syntactically complex** sentences in **agrammatism**.
- Outline basic principles of **assessment** and **rehabilitation** (focus, intensity, compensation vs restitution).

---

## 📖 Required Reading
- **Traxler (1st ed.), Chapter 13, pp. 485–501** — *Aphasia* (overview; classic syndromes; lesion–symptom logic; introductory assessment examples).

---

## 🔑 Key Concepts &amp; Mini-Explanations

### 🧭 Classical Syndromes (behavioral signatures)
- **Broca’s aphasia (nonfluent/agrammatic)**: **Effortful**, short utterances; reduced **function words**/**inflections** (“telegraphic speech”); **relatively better comprehension** for simple sentences; **poor repetition**; **naming** impaired.
- **Wernicke’s aphasia (fluent/logorrheic)**: **Fluent but empty** speech; **semantic paraphasias** (saying *fork* for *spoon*), **neologisms**; **poor comprehension**; **poor repetition**; **naming** impaired.
- **Conduction aphasia**: **Fluent** output and **good comprehension**, but **markedly poor repetition**, especially for **novel** or **long** sequences; frequent **phonemic paraphasias** and **self-corrections**.

### 🧱 WLG Model (and why it’s not the whole story)
- **WLG** posits: Wernicke’s (word meaning/lexicon) ↔ **arcuate fasciculus** ↔ Broca’s (speech planning/syntax). 
- **Limitations**: Real strokes often affect **multiple** regions/white-matter tracts; similar symptoms can arise from **different** lesions; some “Broca’s” symptoms can occur **outside** Broca’s gyrus (and vice versa). Modern views emphasize **distributed networks**.

### 🧪 Naming, Repetition, Comprehension
- **Naming** taps lexical retrieval and phonological encoding; frequency and semantic cues modulate success. 
- **Repetition** stresses **phonological loop** + **arcuate fasciculus** integrity; conduction aphasia shows **repetition-specific** breakdown. 
- **Comprehension** can fail selectively for **syntactic movement/complexity** (e.g., **object relatives**, **passives**) in **agrammatism**.

### 🧩 Agrammatic Comprehension (Broca’s profile)
- Disproportionate difficulty with **non-canonical** word orders: 
 - *“The boy that the girl pushed was tall.”* (object-relative) 
 - *“The boy was pushed by the girl.”* (passive) 
- Performance improves when **semantic**/world knowledge can rescue the parse.

### 🧬 Lesion–Symptom Mapping (intro)
- Beyond single areas: symptoms often reflect **network disconnection** (gray + **white matter**). 
- **VLSM** (voxel-based lesion–symptom mapping) aggregates many patients to identify **regions** where damage **predicts** a deficit better than chance.

### 🔁 Recovery &amp; Rehabilitation (preview)
- Recovery depends on **lesion size/site**, time post-onset, and **therapy intensity**; approaches include **impairment-focused** (e.g., phonological/semantic drills) and **communication-focused** (compensatory strategies). Some therapies leverage **prosody** and **melody** to scaffold speech output.

---

## 📝 Pre-Class Activities
1. **Read** pp. 485–501; list **three** behavioral markers that distinguish **Broca** vs **Wernicke** aphasia. 
2. Watch a short aphasia interview (link in LMS) and note **fluency**, **content**, **comprehension**, **repetition**. 
3. **Sentence probe**: Predict which sentences (active vs passive vs object-relative) will challenge an **agrammatic** patient—and **why**.

---

## 💬 In-Class Activities

### 1) 📺 Case Vignette Spotlights (12 min)
- Listen to short audio/video clips (transcripts provided). 
- In pairs, classify each case (Broca/Wernicke/conduction) and **underline** evidence (fluency, comprehension, repetition, naming).

### 2) 🗺️ Lesion Mapping Puzzle (15 min)
- You receive simplified **MRI silhouettes** with highlighted regions (peri-Sylvian areas, arcuate tract sketch). 
- Match each case to the **most plausible** lesion **pattern**; explain mismatches where behavior and simple localization **diverge**.

### 3) 🔁 Repetition vs Paraphrase (10 min)
- Try to repeat increasing-length sentences; then paraphrase **meaning** only. 
- Discuss why **conduction aphasia** preserves gist but loses **verbatim** form.

### 4) 🧩 Agrammatic Comprehension Lab (10 min)
- **Sentence–picture** matching: actives, passives, object-relatives. 
- Predict error patterns for agrammatic profiles; relate to **syntactic movement** and **working memory**.

### 5) 🧪 Naming Microlab (8 min)
- **High vs low-frequency** pictures with **semantic cueing** (category) or **phonemic cueing** (first sound). 
- Compare facilitation patterns; what does each cue tell us about the **locus** of difficulty?

### 6) Wrap (5 min)
- Exit slip: Name **one** behavior that best distinguishes **conduction** from **Wernicke** aphasia, and **why**.

---

## 🔁 Post-Class Review
- **One-pager**: For one vignette, write a **defensible diagnosis** and a short **lesion hypothesis** (region/tract). 
- **Reflection (100–120 words)**: Which comprehension test best reveals **agrammatism**, and what alternative explanation must you **rule out**?

---

## 🏠 Homework
- **Textbook “Test Yourself”** (Ch. 13, pp. 485–501) items on classic syndromes and repetition/naming patterns. 
- **Short write-up (≈150–200 words)**: Propose a **mini assessment battery** (3–4 tasks) to differentiate **Broca**, **Wernicke**, and **conduction** aphasia in 10 minutes; justify each task.

---

## 🧩 Self-Check Questions

**Q1.** What behavioral triad distinguishes **Broca’s** from **Wernicke’s** aphasia? 
&lt;!-- Broca: nonfluent/effortful speech, relatively better simple comprehension, poor repetition; Wernicke: fluent but empty speech, poor comprehension, poor repetition. -->
&lt;!--
**Q2.** What is the **hallmark** of **conduction aphasia**? 
&lt;!-- Disproportionately impaired repetition with relatively preserved comprehension and fluent output, plus frequent phonemic paraphasias/self-corrections. -->
&lt;!--
**Q3.** Why does a **strict WLG** mapping often fail to predict symptoms perfectly? 
&lt;!-- Real lesions disrupt networks (gray + white matter); similar behaviors can arise from different lesion patterns; functional reorganization/variability across individuals. -->
&lt;!--
**Q4.** Which sentence types typically challenge **agrammatic** comprehension, and why? 
&lt;!-- Non-canonical structures involving movement (passives, object-relatives) because they require hierarchical dependencies beyond linear order. -->
&lt;!--
**Q5.** How can **cueing** help diagnose **naming** deficits? 
&lt;!-- Phonemic cues suggest phonological retrieval problems; semantic cues suggest access to meaning is fragile but can be boosted. -->
&lt;hr>
&lt;!--
## 🧰 Key Terms
**Aphasia**, **Broca’s aphasia**, **Wernicke’s aphasia**, **Conduction aphasia**, **Paraphasia (semantic/phonemic)**, **Agrammatism**, **Repetition**, **Arcuate fasciculus**, **Lesion–symptom mapping**, **VLSM**, **Telegraphic speech**, **Neologism**, **Cueing**.

---

## 🌐 Optional Resources
- **AphasiaBank** (case videos &amp; protocols). 
- NIH/NIDCD overview on **Aphasia** for general background. 
- Short explainer videos on **Broca/Wernicke/conduction** profiles and **sentence–picture** tasks.

---

### ✅ How to use these notes
- **Before class:** skim the vignette transcript, pre-label likely features (fluency, comprehension, repetition). 
- **During class:** argue for a diagnosis using **specific behavioral evidence**. 
- **After class:** compare your lesion hypothesis to the class discussion and revise your **assessment battery**.

-->
&lt;!--
## 🎯 Overview

This week is dedicated to showcasing your work in a collaborative and supportive environment. You will present your final project—either as a **group** or **individual**—in the form of a poster-style summary or a short oral presentation. The goal is to synthesize key insights from the course and apply them to a specific topic or question in psycholinguistics.

---

## 📘 Core Activities

- Deliver a **presentation** (5–7 minutes) based on your project
- Participate in **peer feedback and discussion**
- Learn from others' research ideas and approaches

---

## 👥 Group or Individual Format

Students may present:
- **Group Projects**: Collaborative investigations or literature reviews
- **Individual Projects**: Personal research interests, pilot studies, or reflections

If unsure about format, consult the instructor ahead of time.

---

## ❓ Key Questions to Address

- What is the central **research question** or topic?
- How does your project relate to psycholinguistic theory or methods?
- What did you discover, learn, or hypothesize?
- What challenges did you face, and how did you address them?
- What are potential **future directions**?

---

## 🗣️ Presentation Guidelines

- **Keep it concise**: 5–7 minutes max.
- Focus on **clarity**, **structure**, and **visuals**.
- Your talk/poster should cover:
 - Title &amp; authors
 - Motivation &amp; background
 - Research question(s)
 - Method/approach
 - Key findings or takeaways
 - Final reflections or open questions

---

## 🎯 Evaluation Criteria

Presentations will be graded according to the following rubric:

| Category | Points |
|----------------------------------|--------|
| Clear explanation of topic | 5 |
| Relevance to psycholinguistics | 5 |
| Logical structure and flow | 5 |
| Visual clarity (if slides/poster)| 5 |
| Engagement &amp; delivery | 5 |
| Thoughtful handling of Q&amp;A | 5 |
| **Total** | **30** |

Bonus points may be awarded for originality, creativity, or effective use of examples.

---

## 🤝 Peer Feedback Tips

When watching other presentations, consider:
- What aspect of the project stood out?
- How could the project be extended or improved?
- Is the presentation clear and engaging?
- What’s one follow-up question you would ask?

---

## 📝 No Assigned Reading This Week

Instead, focus on:
- Preparing your slides or poster
- Rehearsing your presentation
- Reviewing other students’ projects with curiosity and kindness

---

## 🧠 Looking Ahead

Next week (Week 17) will feature:
- A course wrap-up discussion
- Optional reflection essays
- Final project write-up submissions (if required)
- Course evaluations

--></description></item><item><title>Week 14: 🌐 Bilingual Minds I</title><link>https://zjpsycholin.github.io/psycholinguistics/week14/</link><pubDate>Sun, 15 Jun 2025 10:03:44 -0400</pubDate><guid>https://zjpsycholin.github.io/psycholinguistics/week14/</guid><description>&lt;p>To be published &amp;hellip;&lt;/p>
&lt;!--
## 📘 Overview
Bilinguals are **not two monolinguals** in one brain. During reading and listening, evidence shows **both languages are active at once**. This yields **cognate facilitation** (e.g., *piano* EN–IT) and **interlingual homograph/false-friend interference** (e.g., Dutch *room* = “cream”, not English *room*). We’ll build an intuition for **nonselective lexical access**, examine the **BIA+ model** (an integrated lexicon with language cues and top-down control), and discuss classic debates like **concept mediation vs word association** in translation. You’ll run mini-demos (Stroop; cognate decision) and sketch how **task demands, script, and proficiency** shape activation.

---

## 🎯 Learning Goals
By the end of Week 14, you should be able to:

- Explain **nonselective lexical access** and predict **cognate** vs **false-friend** effects.
- Describe the architecture and predictions of **BIA+** (feature/letter/word layers, **language nodes**, task/decision components).
- Contrast **concept mediation** vs **word association** in bilingual translation and when each is likely to dominate.
- Discuss how **proficiency**, **age of acquisition**, **script similarity**, and **language mode** (monolingual ↔ bilingual) modulate cross-language activation.
- Interpret basic outcomes from **lexical decision**, **picture naming**, and **Stroop**-type tasks in bilinguals.

---

## 📖 Required Reading
- **Traxler (1st ed.), Chapter 11, pp. 415–431** — *Bilingual Language Processing* (representation, nonselective access, cognates/false friends, BIA+; translation and conceptual mediation).

---

## 🔑 Key Concepts &amp; Mini-Explanations

### 🌊 Nonselective Lexical Access
- During word recognition, **candidates from both languages** become active if they share **form** (orthography/phonology) and/or **meaning**.
- **Cognate facilitation**: shared form + meaning → faster recognition (e.g., EN–ES *hospital*). 
- **Interlingual homographs / false friends**: same form, different meaning → **competition/interference** (e.g., EN *pie* vs ES *pie* “foot”).
- Cross-language activation occurs in **comprehension** and **production** (e.g., **picture naming** slower for bilinguals vs monolinguals due to competition).

### 🧭 BIA+ (Bilingual Interactive Activation Plus)
- **Integrated lexicon** with **feature/letter/word** layers for both languages; **language nodes** index language membership.
- Activation spreads **bottom-up** (features → letters → words) and **laterally** (competition) with **top-down** influences (task context, expectations).
- Predicts: **cognate advantage**, **false-friend cost**, and **task/context** sensitivity (e.g., language-specific cues reduce but rarely eliminate cross-language activation).

### 🔁 Concept Mediation vs Word Association
- **Concept mediation**: L2 word → **concept** → L1 word (dominant in proficient bilinguals). 
- **Word association**: L2 word → **L1 translation** link without full conceptual access (more in beginners or time-pressured tasks). 
- Translation **asymmetries**: often **faster L1→L2** naming than L2→L1 (learning pathways; control demands).

### 🧰 Task &amp; Learner Factors
- **Proficiency/AoA**: higher proficiency → more **direct concept access**, reduced reliance on L1 associations. 
- **Script similarity**: same-script pairs (EN–NL) show **strong orthographic** cross-talk; cross-script (ZH–EN) still show **phonological/semantic** co-activation. 
- **Language mode** (Grosjean): monolingual ↔ bilingual continuum; mode shifts **baseline activation** of each language.

---

## 📝 Pre-Class Activities
1. **Read** pp. 415–431 and list **two cognates** and **two false friends** for your language pair(s). 
2. **Prediction sheet**: For each of your four examples, predict **faster/slower** effects in **lexical decision** and **naming**, and **why** (BIA+ reasoning). 
3. **Mini-Stroop prep**: Review color words in L1 and L2; be ready for a quick response-time challenge.

---

## 💬 In-Class Activities

### 1) 🎯 Stroop (Bilingual Mode) — 10 min
- Slides display color words in **L1/L2** with **ink color** mismatches (e.g., EN *GREEN* printed in red; or L2 word in L1 context). 
- Students respond **ink color**, not word. 
- **Debrief**: Which pairings slowed you down? Relate to **automatic lexical activation** across languages.

### 2) 🔤 Cognate vs Noncognate Decision — 12 min
- Paper/online set includes **cognates**, **noncognates**, and **false friends**. 
- Mark as quickly/accurately as possible whether the item “means the same in both languages.” 
- **Discussion**: Expect **cognate advantage**; identify **false-friend traps** and tie to **competition** in BIA+.

### 3) 🧠 Picture Naming &amp; Competition — 12 min
- Rapid picture naming (L2). Insert **auditory/visual distractors**: L1 translation (facilitates? interferes?), semantically related, or unrelated. 
- **Prediction**: Translation equivalents can **prime** but also **compete**; semantically related distractors tend to **slow** naming via lexical competition.

### 4) 🗺️ BIA+ Whiteboard Build — 10 min
- In groups, draw **BIA+** for a target like EN *piano* / IT *piano*: features → letters → words; show **lateral inhibition** and **language nodes**. 
- Add **task/decision** layer (language cue, context) and propose **one manipulation** to reduce cross-language activation.

### 5) 🧪 Concept Mediation vs Word Association — 8 min
- Translate **L2→L1** and **L1→L2** low-frequency words under **speed** vs **accuracy** instructions. 
- **Debrief**: When did you feel a **direct concept route** vs a **via-L1** route?

### 6) Wrap — 3 min
- Exit slip: one **real-world scenario** where nonselective access is helpful (or harmful) for you.

---

## 🔁 Post-Class Review
- **One-pager**: Choose one demo and explain its outcome using **BIA+** (where activation/competition arose, and the role of **language nodes**). 
- **Reflection (100–120 words)**: How do **script differences** (e.g., ZH–EN) change which cross-language **cues** are strongest for you?

---

## 🏠 Homework
- **Textbook “Test Yourself”** (Ch. 11, pp. 415–431) on **cognates/false friends** and **nonselective access**. 
- **Short write-up (≈150–200 words)**: Design a **lexical decision** mini-study with **cognates**, **false friends**, and **controls**. Specify **predictions** for each item type and **how BIA+** explains them. 
- **Optional**: Keep a two-day **code-mixing diary**; note contexts that nudge you into a more **bilingual mode**.

---

## 🧩 Self-Check Questions

**Q1.** What is **nonselective lexical access** in bilinguals? 
&lt;!-- Activation of lexical candidates from both languages during recognition, driven by shared form/meaning, even when only one language is intended. -->
&lt;!--
**Q2.** Why do **cognates** speed recognition while **false friends** slow it? -->
&lt;!-- Cognates share form and meaning → converging activation; false friends share form but conflict in meaning → competition/interference in the integrated lexicon. -->
&lt;!--
**Q3.** In **BIA+**, what roles do **language nodes** and **task/decision** components play? -->
&lt;!-- Language nodes index language membership and bias competition; task/decision settings (e.g., language context) modulate selection without fully blocking cross-language activation. -->
&lt;!--
**Q4.** When is **concept mediation** more likely than **word association** in translation? -->
&lt;!-- With higher proficiency, deeper semantic tasks, and more time; beginners or speeded tasks may rely on direct L2→L1 links. -->
&lt;!--
**Q5.** How can **script similarity** change cross-language effects? -->
&lt;!-- Same-script pairs amplify orthographic competition (more false-friend interference); cross-script pairs show less orthographic crosstalk but still phonological/semantic co-activation. -->
&lt;hr>
&lt;!--
## 🧰 Key Terms
**Nonselective access**, **Cognate facilitation**, **Interlingual homograph / false friend**, **BIA+**, **Language node**, **Lexical competition**, **Concept mediation**, **Word association**, **Translation asymmetry**, **Language mode**, **Picture-word interference**, **Stroop**.

---

## 🌐 Optional Resources
- Simple online **Stroop** tasks; bilingual false-friend lists for your L1–L2 pair. 
- Short explainers on **BIA+** and **cognate effects** (introductory articles/videos).

---

### ✅ How to use these notes
- **Before class:** prepare your **cognate/false-friend** list and predictions. 
- **During class:** always explain outcomes in terms of **activation/competition** and **BIA+**. 
- **After class:** sketch your own **BIA+** for one tricky false friend and propose a **task tweak** to reduce interference.

-->
&lt;!--
## 📘 Overview

This week we explore how bilinguals process and control their two (or more) languages. We look at how both languages are active during listening and speaking, how interference is managed, and what cognitive advantages might arise from bilingual experience. We also examine individual differences in language learning and the neural basis of bilingualism.

---

## 🧠 Core Topics

### 🌐 Simultaneous Language Activation

- Bilinguals often **activate both languages** during comprehension and production.
- This leads to **competition**, especially when words share meaning or sound.
- Context (e.g., setting, speaker, topic) helps resolve competition.

### 🏗️ Shared vs. Separate Representations

- Bilinguals share **semantic representations**, but may have **distinct phonological** and **orthographic** forms.
- **Shared syntactic structures** suggest deep overlap between languages.

### ⚖️ Language Control Models

- **BIA+ (Bilingual Interactive Activation Plus)**: explains recognition across languages using a layered system.
- **Inhibitory Control Model**: bilinguals suppress the non-target language using cognitive control.

### 🧠 Bilingualism and Executive Function

- Bilinguals show **enhanced cognitive control**:
 - Inhibitory control
 - Task switching
 - Working memory
- Evidence includes **Stroop tasks** and **Simon effect** studies.

### 🎓 Second Language Learning and Variability

- Factors influencing success:
 - **Age of acquisition**
 - **Language similarity**
 - **Motivation**
 - **Exposure and use**
- Teaching methods (e.g., immersion vs. classroom) affect outcomes.

### 🧬 Neural Basis of Bilingualism

- L1 and L2 activate overlapping brain areas.
- **Proficiency** and **age of acquisition** modulate brain patterns.
- Brain adapts with experience, not just innate capacity.

---

## 🧪 In-Class Activities

### 🧠 Bilingual Stroop Task

- Demo: bilingual students perform Stroop task in both languages.
- Compare reaction times and discuss cognitive flexibility.

### 🗣️ Interference Simulation

- Pair up: one speaks only L1, the other only L2 during a task.
- Reflect on interference, switching, and control.

### 📈 Debate: Is Bilingualism an Advantage?

- One group defends cognitive benefits, another challenges with alternative explanations.
- Use experimental findings to support arguments.

---

## ❓ Key Questions

1. How do bilinguals manage interference from both languages?
2. What are the main cognitive consequences of bilingualism?
3. How do models like BIA+ explain bilingual word recognition?
4. What factors influence second language learning success?

---

## 🏷️ Key Terms

| Term | Definition |
|------|------------|
| **Language competition** | Simultaneous activation of multiple language systems |
| **Inhibitory control** | Ability to suppress one language while using another |
| **Executive control** | Higher-order cognitive functions enhanced in bilinguals |
| **BIA+** | A model of bilingual word recognition with interactive layers |
| **Critical period** | Hypothesized window during which L2 learning is most successful |

---

## 📚 Reading

- Traxler (2012), Chapter 11: *Bilingual Language Processing* (pp. 415–438)

---

## 📝 Practice Prompt

> A Spanish-English bilingual sees the word “pie.” 
> - What language is activated first?
> - How might context help resolve ambiguity?

---

## 🔁 Related Weeks

- Week 12–13: *Language Development*
- Week 15: *Sign Language and Multimodal Communication*
--></description></item><item><title>Week 15: 🌐 Bilingual Minds II</title><link>https://zjpsycholin.github.io/psycholinguistics/week15/</link><pubDate>Sun, 15 Jun 2025 10:03:44 -0400</pubDate><guid>https://zjpsycholin.github.io/psycholinguistics/week15/</guid><description>&lt;p>To be published &amp;hellip;&lt;/p>
&lt;!--
## 📘 Overview
This week extends our view of bilingual processing to **language control and switching**. We examine why switching from **L2→L1** can be **harder** than L1→L2 (the **asymmetric switch-cost** pattern), when costs can become **symmetric** (proficiency/task context), and what **code-switching** reveals about control in natural conversation. We also look at **structural priming across languages** as evidence for **shared syntactic representations**, and we connect behavior to the **brain** using core findings from **fMRI/ERP** on bilingual control.

---

## 🎯 Learning Goals
By the end of Week 15, you should be able to:

- Describe **asymmetric switch costs** and explain them using an **inhibitory-control** account.
- Identify conditions that yield **symmetric** or reduced switch costs (e.g., high proficiency, intense mixing mode).
- Explain **reversed dominance** (L2 outperforming L1 under strong L1 inhibition).
- Summarize evidence for **shared syntax** via **cross-language structural priming**.
- Interpret basic **neuroimaging** signatures of bilingual control (ACC, DLPFC, basal ganglia; ERP N2/P3 modulation).
- Analyze short **code-switching** samples for control demands, triggers, and comprehension effects.

---

## 📖 Required Reading
- **Traxler (1st ed.), Chapter 11, pp. 431–447** — *Bilingual Language Processing* (language control, switching costs, code-switching, structural priming across languages, neural evidence).

---

## 🔑 Key Concepts &amp; Mini-Explanations

### 🔄 Language Switching &amp; Control
- **Asymmetric switch costs**: Switching **L2→L1** is often **slower** than L1→L2 because L1 must be **suppressed** to allow L2 production; lifting strong L1 inhibition is costly. 
- **Symmetric costs** can emerge with **balanced proficiency**, heavy **mixing**, or tasks that minimize L1 dominance.
- **Reversed dominance**: In some mixed-language contexts, performance can be **better in L2** than L1 if L1 remains **strongly inhibited** across trials.

### 🧠 Inhibitory Control &amp; Alternatives
- **Inhibitory Control view**: A domain-general control system down-regulates the **non-target** language to reduce competition; switch trials require **reconfiguration**, yielding costs.
- **Task factors** (cue reliability, response deadlines, stimulus type) and **learner factors** (proficiency, AoA) modulate how much inhibition is applied.

### 🗣️ Code-Switching (Naturalistic)
- Switching reflects **control** interacting with **pragmatics** (addressee, topic, emphasis) and **availability** (recent activation, lexical access). 
- Comprehenders usually keep both languages partially active; **integration** is smoother when switches align with **phrase boundaries** and predictable **discourse** points.

### 🧩 Structural Priming Across Languages
- Producing/comprehending a structure in one language (e.g., **passive**) increases the chance of using/comprehending the **same structure** in the other language shortly after. 
- Supports **shared** or **closely linked** **syntactic representations**, not two entirely separate grammars in use.

### 🧲 Brain Signatures of Control
- **fMRI**: **Anterior cingulate cortex (ACC)** and **dorsolateral prefrontal cortex (DLPFC)** recruit during conflict/monitoring; **basal ganglia/caudate** implicated in language selection/switching. 
- **ERP**: Increased **N2** (conflict/monitoring) and **P3** (updating) on switch or conflict trials; component sizes modulated by **proficiency** and **context**. 
- With experience, neural responses can show **efficiency** (reduced effort for common switches).

---

## 📝 Pre-Class Activities
1. **Read** pp. 431–447 and list: one example of **asymmetric cost**, one **code-switch** you’ve heard/used, and one **imaging or ERP** finding. 
2. **Prediction sheet**: Will you show **reversed dominance** in a mixing task? Why (based on your L1/L2 strengths)? 
3. **Priming prep**: Write two sentence pairs (EN↔ZH or your L2) using the **same structure** (e.g., active/passive) to test **cross-language priming**.

---

## 💬 In-Class Activities

### 1) 🔄 Color-Cued Switching (12 min)
- Name digits/pictures; **blue = L1**, **green = L2**. Measure **switch** vs **stay** trials (paper timing or browser timer). 
- **Debrief**: Do you see **L2→L1** cost > L1→L2? Any **reversed dominance**?

### 2) 🧪 Translation Stroop (10 min)
- Name ink color while words are **color terms** in **L1 or L2** (or translations). 
- Expect **interference** when word meaning conflicts with ink; compare L1 vs L2 magnitudes.

### 3) 🧠 Structural Priming Across Languages (12 min)
- Read/produce a prime sentence in **Language A**; immediately produce a description in **Language B**. 
- Tally **same-structure** uses; discuss how this supports **shared syntax**.

### 4) 🗺️ Code-Switch Clinic (12 min)
- Short transcribed snippets (2–3 switches each). Label **switch points** and infer **control demands** (lexical retrieval, emphasis, addressee shift). 
- Note when a switch **helps** (retrieval) vs **hurts** (integration cost).

### 5) 🧠 Brain Map Quickmatch (11 min)
- Match findings to regions/components: **ACC**, **DLPFC**, **basal ganglia/caudate**, **N2**, **P3**. 
- Propose one reason **proficiency** would **reduce** activation for a given contrast.

### 6) Wrap (3 min)
- Exit slip: one **task tweak** that would **reduce** your switch costs.

---

## 🔁 Post-Class Review
- **One-pager**: Explain your switching results with **Inhibitory Control** (where inhibition was applied, how it produced asymmetry/symmetry). 
- **Reflection (100–120 words)**: In which real settings do you **choose** to code-switch? Does it feel like effort or relief? Why?

---

## 🏠 Homework
- **Textbook “Test Yourself”** (Ch. 11, pp. 431–447) on **switching**, **control**, and **priming**. 
- **Short write-up (≈150–200 words)**: Design a **language-switching** task that tests for **reversed dominance**. Specify **cues**, **trial structure**, and **predictions** for high- vs low-proficiency bilinguals. 
- **Optional**: Keep a 24-hour **code-switch diary**; categorize each switch (addressee/topic/lexical retrieval).

---

## 🧩 Self-Check Questions

**Q1.** Why is **L2→L1** often slower than **L1→L2** in switching tasks? 
&lt;!-- Because L1 is strongly inhibited to allow L2 production; removing that inhibition on a switch back to L1 is costly (asymmetric switch cost). -->
&lt;!--
**Q2.** Name a condition that can make switch costs **symmetric**. -->
&lt;!-- Higher/balanced proficiency or sustained mixed-language context that reduces L1 dominance (both languages similarly activated). -->
&lt;!--
**Q3.** What is **reversed dominance**? -->
&lt;!-- When performance in L2 surpasses L1 under persistent L1 inhibition in mixed-language contexts. -->
&lt;!--
**Q4.** How does **cross-language structural priming** support **shared syntax**? --> 
&lt;!-- Using the same structure across languages after a prime suggests linked or shared syntactic representations rather than fully separate grammars in use. -->
&lt;!--
**Q5.** Which regions/components are commonly linked to **bilingual control** and **switching**? -->
&lt;!-- ACC/DLPFC (conflict/monitoring), basal ganglia/caudate (selection/switching), ERP N2 (conflict) and P3 (updating). -->
&lt;hr>
&lt;!--
## 🧰 Key Terms
**Asymmetric switch cost**, **Inhibitory Control**, **Reversed dominance**, **Language mode**, **Code-switching**, **Structural priming**, **Shared syntax**, **ACC**, **DLPFC**, **Basal ganglia/caudate**, **N2**, **P3**.

---

## 🌐 Optional Resources
- Simple browser-based **switching** and **Stroop** demos. 
- Short primers on **ERP components** (N2/P3) and **frontostriatal** circuits in control. 
- Brief guides on analyzing **code-switch** transcripts.

---

### ✅ How to use these notes
- **Before class:** prepare your switch-task predictions and prime sentences. 
- **During class:** always connect behavior to **control mechanisms** and **proficiency/context**. 
- **After class:** sketch a mini-study to test one **manipulation** (e.g., stronger language cues) that should **shrink** switch costs.

-->
&lt;!--
## 📘 Overview

This week we examine sign languages as fully expressive natural languages. We discuss their phonological, morphological, and syntactic structures; how they are processed in the brain; and how they are acquired. We also explore cognitive consequences of deafness, the role of the right hemisphere, and issues related to cochlear implants.

---

## 🧠 Core Topics

### ✋ Characteristics of Signed Languages

- Sign languages have **phonological parameters** (handshape, location, movement).
- They exhibit **morphological complexity**, including rich inflectional morphology.
- Syntax in signed languages includes **word order variation** and **use of space for grammatical relations**.

### 🧠 Lexical Access and Processing

- Signers exhibit similar effects as spoken language users in **lexical access** (e.g., frequency, neighborhood density).
- **Priming** effects and real-time processing are evident in ERP and reaction time studies.

### 👶 Language Acquisition and Evolution

- Deaf children exposed to sign language from birth acquire it on a similar timetable as spoken languages.
- Sign language use sheds light on **language evolution** through gesture.

### 📖 Reading in Deaf Signers

- Reading outcomes depend on **early language exposure**.
- Proficiency in sign language correlates positively with **reading ability**.

### 🧠 Neural Basis of Sign Language

- Sign language is **left-lateralized**, much like spoken language.
- **Right hemisphere** contributes to processing **spatial and visual features**.
- fMRI and lesion studies show activation in traditional **Broca’s and Wernicke’s areas**.

### 🎧 Cochlear Implants

- Implants restore partial hearing but outcomes vary:
 - Early implantation leads to better results.
 - Language input before implantation matters.
- Language development after CI use differs depending on **prior sign language use**.

---

## ❓ Key Questions

1. How does sign language compare structurally and cognitively to spoken language?
2. What brain regions are involved in sign language processing?
3. How do deaf children acquire sign language, and how does it affect reading development?
4. What are the implications of cochlear implants for language development?

---

## 🧪 In-Class Activities

### 🧏 Sign Language Video Analysis

- Watch clips of native signers (ASL, BSL, CSL).
- Identify morphological markers and syntactic structure.
- Discuss how information is spatially represented.

### 🧠 Lateralization Mapping

- Examine brain imaging studies comparing signers and speakers.
- Match language functions to neural regions.

### 🎭 Gesture Communication Game

- Students use only gesture to describe actions or concepts.
- Reflect on expressiveness and limitations of visual modality.

---

## 🏷️ Key Terms

| Term | Definition |
|------|------------|
| **Phonological parameters** | Features such as handshape and location in sign |
| **Spatial grammar** | Use of space to encode grammatical relations |
| **Left lateralization** | Dominance of left hemisphere for language processing |
| **Cochlear implant** | Device that enables sound perception in profoundly deaf individuals |
| **Perspective-taking** | Cognitive skill linked to sign language and visual processing |

---

## 📚 Reading

- Traxler (2012), Chapter 12: *Sign Language* (pp. 447–472)

---

## 📝 Practice Prompt

> Compare the structure of a sign language sentence with a spoken language sentence. What are the modality-specific strategies used to express grammatical relations?

---

## 🔁 Related Weeks

- Week 14: *Bilingual Language Processing*
- Week 16: *Student Presentations and Integration*
--></description></item><item><title>Week 13: 🧠 Language Development II</title><link>https://zjpsycholin.github.io/psycholinguistics/week13/</link><pubDate>Thu, 12 Jun 2025 10:03:44 -0400</pubDate><guid>https://zjpsycholin.github.io/psycholinguistics/week13/</guid><description>&lt;p>To be published &amp;hellip;&lt;/p>
&lt;!--

## 📘 Overview
How do children move from isolated words to **structured vocabulary and grammar**? This week we examine **word-learning biases** (whole-object, basic-level), **mutual exclusivity (ME)/contrast**, **fast mapping**, and **syntactic bootstrapping** (using sentence frames to infer meaning). We then tackle **morphology**: productivity, **over-regularization** (the classic **U-shaped** curve), and the **Wug test**. We’ll compare **rule-based** and **probabilistic/connectionist** accounts and consider English–Mandarin differences (e.g., **count vs mass**, classifiers).

---

## 🎯 Learning Goals
By the end of Week 13, you should be able to:

- Explain **fast mapping**, **mutual exclusivity**, and **principle of contrast** and predict their effects in word learning tasks.
- Use **syntactic bootstrapping** to infer meanings for **nouns vs verbs**, including **transitivity** and **count vs mass** cues.
- Describe and graph the **U-shaped** pattern in **past-tense** and plural learning; distinguish **over-** vs **underextension**.
- Design a simple **Wug-style** test for a new inflection and justify what counts as evidence of **productivity**.
- Compare **words-and-rules** vs **probabilistic/connectionist** explanations for morphology and evaluate evidence for each.
- Identify **cross-linguistic** implications (Mandarin classifiers, verb-friendly input) for acquisition pathways.

---

## 📖 Required Reading
- **Traxler (1st ed.), Chapter 9, pp. 344–360** — *Language Development* (word learning, bootstrapping, morphology).

---

## 🔑 Key Concepts &amp; Mini-Explanations

### ⚡ Fast Mapping &amp; Biases
- **Fast mapping**: forming a partial, durable link between a novel word and a referent after minimal exposure.
- **Whole-object bias**: new labels tend to name **whole objects** rather than parts or properties.
- **Basic-level bias**: children prefer **basic-level** categories (dog) over superordinate (animal) or subordinate (poodle).

### 🧩 Mutual Exclusivity (ME) &amp; Principle of Contrast
- **ME**: children assume **one label per object**; a novel word is mapped to the **unnamed** object/property.
- **Contrast**: different words signal **different meanings**; supports **fine-grained** distinctions.
- Caveat: ME can be **overridden** by context (bilingual input, part/shape/property tasks).

### 🧠 Syntactic Bootstrapping
- Children use **sentence structure** to infer meaning:
 - **Transitivity**: *“She **blicked** the ball”* (two arguments) → likely **causative** verb; *“She **blicked**”* (one argument) → likely **intransitive**.
 - **Count vs mass**: *“a **sib**”* (count noun frame) vs *“some **sib**”* (mass noun frame) → predicts **thing** vs **substance** interpretation.
 - **Noun vs verb** frames in Mandarin/English: word order, aspect markers, and classifiers provide additional **syntactic/semantic** cues.

### 🧱 Over/Underextension
- **Overextension**: applying a word too **broadly** (e.g., “dog” for all four-legged animals).
- **Underextension**: applying a word too **narrowly** (e.g., “bottle” only for a specific cup at home).

### 🧪 Morphology: Productivity &amp; the Wug Test
- **Productivity**: applying a rule to **novel** items (e.g., *wug → wugs*).
- **Wug test** probes whether children **generalize** beyond memorized forms.

### 📉 Over-regularization &amp; the U-Shaped Curve
- Early **correct** (memorized irregulars) → middle **over-regularization** (*goed*, *mouses*) → later **recovery** as **rules + exceptions** are consolidated.
- Reflects interaction of **memory strength**, **rule extraction**, and **competition** between patterns.

### 🧮 Accounts of Morphology
- **Words-and-rules (dual-route)**: a **rule** composes regulars; **lexicon** stores exceptions; **race** between rule and memory.
- **Probabilistic/connectionist**: a **single system** that learns graded mappings; “regularity” emerges from **distributional patterns** and **phonology**.
- Evidence to weigh: **nonce-word generalization**, **frequency effects**, **phonological neighborhood**, **error distributions**.

### 🌏 Cross-Linguistic Notes (EN ↔ ZH)
- **Mandarin**: **classifiers** in noun phrases, fewer obligatory plural markings → different path for number marking; richer **aspect** system may scaffold verb learning.
- **English**: **count/mass** morphology and articles provide early **nominal** cues; past-tense suffix **-ed** encourages **rule extraction**.

---

## 📝 Pre-Class Activities
1. **Read** pp. 344–360 and annotate one example each of **ME**, **fast mapping**, **syntactic bootstrapping**, and an instance of **over-regularization**. 
2. **Frames worksheet**: For five novel words, place each into **count/mass** or **transitive/intransitive** frames and write your predicted meanings. 
3. **Family data** (optional): Ask a younger sibling/cousin (or recall) one over- or underextension. Bring a one-sentence description.

---

## 💬 In-Class Activities

### 1) 🎲 Morphological Games: The Mini-Wug (15 min)
- Groups create **3 nonce nouns** and **3 nonce verbs** with varied phonological endings (e.g., *tave, wug, spling*; *norp, chaze, glim*). 
- Class applies **plural** and **past-tense** rules; record **regularization** vs **exception-like** guesses. 
- Discuss: What counts as **evidence of productivity**?

### 2) 🔍 Bootstrapping Simulation (18 min)
- Verb frames on cards: **transitive** vs **intransitive**; clips/descriptions of ambiguous events. 
- Teams map frames → **probable meanings**; then test with **count/mass** noun frames for a new noun. 
- Debrief: Which cues were **most diagnostic**? Where did you **mislead** yourselves?

### 3) ⚡ Fast Mapping &amp; ME Challenge (10 min)
- Show an array with one **familiar** and one **novel** object; introduce a **novel label**. 
- Predict children’s mapping under **ME**; then **override** ME by asking for a **part/property** (e.g., “Show me the **dax** that means the **handle**”). 
- Note when ME **fails** and why.

### 4) 📉 U-Shaped Curve Lab (10 min)
- Plot a **toy timeline** of irregular past-tense accuracy (e.g., *go/went*). 
- Diagnose factors that **raise** or **lower** the middle dip (input frequency, corrective feedback, phonology).

### 5) 🌏 EN–ZH Compare &amp; Contrast (5 min)
- Quick cases: **classifiers** with numerals in Mandarin vs **plural -s** in English; implications for early **number** concepts.

### 6) Wrap (2 min)
- Write one **prediction** about when a child will **over-regularize** a new -ed verb and why.

---

## 🔁 Post-Class Review
- **One-pager**: Choose one **verb** and one **noun** from class; explain how **frames** and **ME/contrast** guided your inference. 
- **Reflection (100–120 words)**: Which account (dual-route vs connectionist) better explains your **Mini-Wug** results? Why?

---

## 🏠 Homework
- **Textbook “Test Yourself”** (Ch. 9, pp. 344–360) on ME/contrast, bootstrapping, morphology. 
- **Short write-up (≈150–200 words)**: Design a **Wug-style** test for an **unfamiliar inflection** (e.g., a fictitious **diminutive**). Specify **items**, **procedure**, **success criteria**. 
- **Optional**: Collect **2 child utterances** (any language) that show **over-** or **underextension** or **over-regularization**; tag them with a **likely cause**.

---

## 🧩 Self-Check Questions

**Q1.** What is **mutual exclusivity**, and when can it be overridden? 
&lt;!-- ME: assume one label per object; overridden by bilingual exposure, explicit part/property requests, or strong contextual evidence. -->
&lt;!--
**Q2.** Give a **syntactic bootstrapping** example for a **verb**. -->
&lt;!-- “The girl daxed the cat” (transitive) implies a two-participant/causative event; “The girl daxed” (intransitive) implies a single-participant action. -->
&lt;!--
**Q3.** Define **over-regularization** and sketch the **U-shaped** pattern. -->
&lt;!-- Applying a regular rule to irregular forms (e.g., goed); early correct → mid over-regularization → later recovery as rules and exceptions stabilize. -->
&lt;!--
**Q4.** What makes a **Wug test** good evidence of **productivity**? -->
&lt;!-- Generalization to truly novel forms with systematic application of the pattern across varied phonologies, not just memorized analogies. -->
&lt;!--
**Q5.** How do **count vs mass** frames guide noun interpretation? -->
&lt;!-- “a N” cues count/individuated entities; “some N” cues mass/substance; in Mandarin, classifiers with numerals support individuation without plural -s. -->
&lt;hr>
&lt;!--
## 🧰 Key Terms
**Fast mapping**, **Whole-object bias**, **Basic-level bias**, **Mutual exclusivity (ME)**, **Principle of contrast**, **Syntactic bootstrapping**, **Transitivity**, **Count vs mass**, **Over/underextension**, **Wug test**, **Productivity**, **Over-regularization**, **U-shaped learning**, **Words-and-rules**, **Connectionist/probabilistic learning**, **Classifier**.

---

## 🌐 Optional Resources
- Short videos on **bootstrapping** demonstrations. 
- Classic **Wug** materials and modern variants for classroom replication. 
- Readings on **bilingual ME** effects and **classifier** systems.

---

### ✅ How to use these notes
- **Before class:** complete the frames worksheet and bring one real or remembered child-language example. 
- **During class:** explicitly **name** the cue (ME/contrast, transitivity, count/mass) behind each decision. 
- **After class:** reflect on which **model** best captured your Mini-Wug outcomes.

-->
&lt;!--
## 📘 Overview

This week focuses on how children develop **morphological** and **syntactic** knowledge in the first few years of life. We will trace the emergence of **inflectional morphology**, **function words**, and **sentence structures**, and examine theories that explain how children achieve such rapid mastery, including **semantic**, **syntactic**, and **prosodic bootstrapping**.

---

## 🧠 Core Topics

### 🔠 Morphological Development

- Early speech includes **single-word** utterances → **multi-word combinations**.
- Emergence of **inflectional morphemes** (e.g., -ed, -s) around age 2.
- **Overregularization errors** show internalization of rules:
 > “goed,” “foots,” “breaked”

- These errors are U-shaped:
 - Stage 1: Memorized forms (“went”)
 - Stage 2: Rule-based overgeneralization (“goed”)
 - Stage 3: Correct rule-plus-exception (“went” again)

---

### 📦 Syntactic Development

- Early utterances are **telegraphic**: “Want juice,” “Daddy go work”
- Gradual inclusion of:
 - **Function words** (e.g., determiners, auxiliaries)
 - **Complex structures** (e.g., questions, negation)

- Developmental sequence:
 1. Word combinations (18–24 mo.)
 2. Subject–verb–object (SVO) structure
 3. Negation and questions
 4. Relative clauses, passives (by ~4–5 years)

---

### 🚼 Bootstrapping Theories

- **Semantic bootstrapping**: Children infer syntactic roles from meaning.
- **Syntactic bootstrapping**: Use sentence structure to infer word meaning.
 > e.g., “She daxed the toy” → “dax” is likely a verb
- **Prosodic bootstrapping**: Use rhythm, intonation to detect structure.

---

### 🗣️ Input and Feedback

- Children’s grammatical development depends on:
 - **Quantity** and **quality** of caregiver input
 - **Recasts** and **expansions** (corrective feedback)
 - Exposure to **varied** and **structured** language

---

## 🧪 In-Class Activities

### 🧠 Overregularization Sorting

- Provide examples of child errors: students classify as overregularized, correct, or irregular.

### 🛠️ Sentence Construction Task

- Students build increasingly complex sentences from word cards.
- Reflect on when and how children acquire these structures.

### 🔄 Bootstrapping Debate

- Divide class into groups: semantic vs. syntactic vs. prosodic bootstrapping.
- Each team defends their theory with examples and evidence.

---

## ❓ Key Questions

1. What are overregularization errors and what do they reveal about grammatical development?
2. How do children acquire function words and syntactic rules?
3. What roles do bootstrapping mechanisms play in grammar learning?
4. How does language input shape syntactic development?

---

## 🏷️ Key Terms

| Term | Definition |
|------|------------|
| **Overregularization** | Applying grammatical rules to irregular forms (e.g., "goed") |
| **Telegraphic Speech** | Early utterances lacking function words |
| **Function Words** | Words like "the," "is," "can" that serve grammatical purposes |
| **Bootstrapping** | Learning one linguistic system using cues from another |
| **Recasts** | Caregiver reformulations of child utterances |

---

## 📚 Reading

- Traxler (2012), Chapter 9: *Language Development in Infancy and Early Childhood* (pp. 344–360)

---

## 📝 Practice Prompt

> Child says: “Doggy runned fast!” 
> - What developmental stage is this?
> - What error type is present?
> - What feedback could a caregiver provide?

---

## 🔁 Related Weeks

- Week 12: *Infants and Early Words*
- Week 14: *Bilingual Language Development*

--></description></item><item><title>Week 12: 👶 Language Development I</title><link>https://zjpsycholin.github.io/psycholinguistics/week12/</link><pubDate>Sun, 08 Jun 2025 10:03:44 -0400</pubDate><guid>https://zjpsycholin.github.io/psycholinguistics/week12/</guid><description>&lt;p>To be published &amp;hellip;&lt;/p>
&lt;!--
## 📘 Overview
Infants don’t get spaces between words. Yet by the end of the first year they already **recognize familiar words** and use multiple **cues** to carve continuous speech into candidates for words. This week we look at **prenatal learning**, **infant-directed speech (IDS/motherese)**, classic **infant methods** (HAS, HPP, CHT), **categorical perception &amp; perceptual narrowing**, and how **statistical learning**, **prosody**, and **phonotactics** help solve the **segmentation problem**.

---

## 🎯 Learning Goals
By the end of Week 12, you should be able to:

- Describe **infant testing methods** (HAS, HPP, CHT) and what each reveals.
- Explain **categorical perception** and **perceptual narrowing** in the first year.
- Identify key properties of **IDS** and discuss how they may support attention and learning.
- Apply **statistical**, **prosodic**, and **phonotactic** cues to segment a speech stream.
- Compare segmentation cues in **English** and **Mandarin** and predict learner challenges.

---

## 📖 Required Reading
- **Traxler (1st ed.), Chapter 9, pp. 325–344** — *Language Development* (infant perception, IDS, segmentation strategies).

---

## 🔑 Key Concepts &amp; Mini-Explanations

### 👶 Prenatal &amp; Newborn Learning
- Fetuses are sensitive to **prosodic patterns**; newborns prefer **mother’s voice** and **native-language rhythm**.
- Early preferences suggest **memory** for speech contours even before birth.

### 🧪 How We Measure Infant Perception
- **HAS (High-Amplitude Sucking)**: increased sucking when hearing something **novel**; used for **discrimination**.
- **HPP (Head-Turn Preference Procedure)**: infants turn toward the side playing stimuli they **prefer** (e.g., familiar vs novel).
- **CHT (Conditioned Head-Turn)**: train infants to turn head when a **phonetic category** changes (e.g., /ba/→/pa/); reveals **category boundaries**.

### 🎚️ Categorical Perception &amp; Perceptual Narrowing
- Young infants discriminate **many** phonetic contrasts (even non-native); by ~10–12 months, sensitivity **narrows** toward **native** categories.
- Tuning is driven by **distributional** exposure (how often sounds appear near category boundaries).

### 🎶 Infant-Directed Speech (IDS / Motherese)
- Typical features: **higher pitch**, **exaggerated intonation**, **slower tempo**, **clearer vowels**, **shorter utterances**, more **repetition**.
- Functions: captures **attention**, highlights **phrase boundaries**, may enhance **category learning**; varies across languages/cultures.

### ✂️ The Segmentation Problem
- Continuous speech lacks consistent **word-boundary pauses**. Learners exploit **multiple cues**:
 - **Statistical learning**: lower **transitional probability** across word boundaries than within words.
 - **Prosodic cues**: **stress** patterns (e.g., English **trochaic** bias: STRONG-weak), **intonation** boundaries.
 - **Phonotactics**: language-specific constraints on **sound sequences** (e.g., where clusters can occur).
 - **Allophonic cues**: subtle sound variants signal **syllable** or **word** edges.
 - **Anchors**: own **name**, frequent **function words**, and familiar **frames** bootstrap segmentation.

### 🗺️ Cross-Linguistic Notes (EN ↔ ZH)
- **English**: stress-timed rhythm, many **consonant clusters**, strong **trochaic** tendency → stress cues are powerful.
- **Mandarin**: **tone** contrasts and relatively **syllable-timed** rhythm; less reliable **lexical stress** → learners rely more on **syllable**/tone and **statistics** than fixed stress.

---

## 📝 Pre-Class Activities
1. **Read** pp. 325–344 and note one example each of **statistics**, **prosody**, and **phonotactics** aiding segmentation. 
2. **Listen** to two short clips (to be provided): **ADS** vs **IDS**. List 3 acoustic differences. 
3. **Mini-stream**: Practice segmenting an artificial stream like *“bidakupadotigolabubidaku…”*. Mark your best guess at **word boundaries**.

---

## 💬 In-Class Activities

### 1) 🎶 Motherese Analysis (12 min)
- In pairs, mark features in IDS vs ADS clips: **pitch range**, **tempo**, **vowel space**, **repetition**.
- Discuss: Which features would most help a **9-month-old** find words?

### 2) 🎵 Saffran-Style Segmentation Lab (18 min)
- Hear a 2-min **continuous** stream composed of 4 trisyllabic “words.” 
- Individually mark boundaries; then, as a class, compute **transitional probabilities** for two syllable pairs and see if your boundaries match the **actual words**.

### 3) 🔤 Strong-Syllable Hunt (10 min)
- English-like strings (e.g., **“MÁla bu** **GÓti pa** **DÁku…”**). Circle **strong** syllables; hypothesize **word onsets**.
- Compare to **Mandarin**: How would a tone language listener approach this without a strong **stress cue**?

### 4) 🔐 Phonotactics Puzzle (8 min)
- Given a mini **phonotactic table** (which onsets/codas are legal), decide where **boundaries** must fall in *“…ktobugla…”* vs *“…sto#bu…”* and justify.

### 5) 🧪 Methods Corner (10 min)
- Groups sketch a simple **HPP** or **CHT** experiment to test sensitivity to a non-native contrast (e.g., dental vs retroflex). Identify **stimuli**, **procedure**, **predictions**.

### 6) Wrap (2 min)
- On a sticky, write one **cue** you plan to rely on first when segmenting a noisy stream (and why).

---

## 🔁 Post-Class Review
- **One-pager**: For your artificial stream, list the cues you used (statistics/prosody/phonotactics), your **final segmentation**, and one **uncertainty**. 
- **Reflection (100–120 words)**: How might **Mandarin** vs **English** input shape early segmentation strategies?

---

## 🏠 Homework
- **Textbook “Test Yourself”** (Ch. 9, pp. 325–344) on IDS and segmentation. 
- **Short write-up (≈150–200 words)**: Design a minimal **HPP** study testing whether infants prefer **familiarized words** over part-words after exposure to an artificial language. Include **materials**, **procedure**, and **prediction**. 
- **Optional**: Record 30 seconds of your own **IDS** (in any language) reading a picture book; annotate where you **pause** and **exaggerate** pitch.

---

## 🧩 Self-Check Questions

**Q1.** What does **HPP** measure and how is it different from **CHT**? 
&lt;!-- HPP indexes listening preference (e.g., longer orientation to familiar/novel items) without conditioning; CHT conditions infants to turn at a category change to test discrimination with clear criteria. -->
&lt;!--
**Q2.** Define **perceptual narrowing** and give a typical time window. --> 
&lt;!-- Early broad sensitivity to many contrasts becomes tuned to native categories around 10–12 months as distributions are learned. -->
&lt;!--
**Q3.** How do **transitional probabilities** signal word boundaries? -->
&lt;!-- Within-word syllable pairs have higher TPs than across-boundary pairs; low TP dips flag likely boundaries. -->
&lt;!--
**Q4.** Why is **IDS** potentially helpful for segmentation? -->
&lt;!-- Higher pitch, exaggerated intonation, slower rate, and clearer vowels enhance attention and may highlight phrase/word boundaries. -->
&lt;!--
**Q5.** Name one segmentation cue likely **stronger in English** than in **Mandarin**, and explain why. -->
&lt;!-- Lexical stress/prosodic stress: English’s trochaic bias provides a robust cue, whereas Mandarin lacks fixed lexical stress and relies more on tone and syllable timing. -->
&lt;hr>
&lt;!--
## 🧰 Key Terms
**High-Amplitude Sucking (HAS)**, **Head-Turn Preference (HPP)**, **Conditioned Head-Turn (CHT)**, **Infant-Directed Speech (IDS)**, **Categorical perception**, **Perceptual narrowing**, **Segmentation**, **Transitional probability**, **Prosodic bootstrapping**, **Phonotactics**, **Allophonic cues**.

---

## 🌐 Optional Resources
- Short explainer videos on **infant speech perception** methods (HAS/HPP/CHT). 
- Interactive demos of **statistical learning** with artificial languages. 
- Brief readings on **prosodic bootstrapping** and **tone vs stress** in infant learning.

---

### ✅ How to use these notes
- **Before class:** try the mini-stream and list the cues you relied on. 
- **During class:** justify each boundary with a **named cue** (statistics, prosody, phonotactics). 
- **After class:** compare your first pass to the revealed words and note which cue would have helped most.

-->
&lt;!--
## 📘 Overview

This week we explore how language acquisition begins **before birth** and progresses rapidly through **infancy**. We examine how infants perceive speech sounds, solve the segmentation problem, and begin learning their first words—all with **minimal explicit instruction**.

---

## 🧠 Core Topics

### 🍼 Prenatal Language Learning

- Fetuses respond to external auditory stimuli (e.g., mother's voice, native language prosody).
- Evidence: **High-amplitude sucking (HAS)** shows preference for stories heard in utero.

---

### 🔍 Speech Perception and Phoneme Categorization

- Infants perceive **phonemic contrasts** not present in their native language (universal perceivers).
- With exposure, perception becomes **language-specific** by 10–12 months.
- Tools: **Habituation** and **head-turn paradigms**.

---

### 🧩 Solving the Segmentation Problem

- Speech is **continuous**; no silences mark word boundaries.
- Infants rely on:
 - **Prosodic cues** (stress patterns)
 - **Phonotactic probabilities**
 - **Statistical learning** (tracking transitional probabilities between syllables)

---

### 🧠 Infant-Directed Speech (IDS)

- Features: Higher pitch, exaggerated intonation, slower rate.
- Helps attract attention and **highlight linguistic structure**.

---

### 🧠 Early Word Learning

- Infants map sounds to meaning via:
 - **Associationist learning**
 - **Cross-situational learning**
 - **Social cues** (e.g., gaze, joint attention)
- **Mutual exclusivity** and **whole object assumption** guide inferences.

---

## 🧪 Class Activities

### 👂 Infant Simulation: Syllable Stream Segmentation

- Play artificial language stream with hidden word boundaries.
- Students try to segment based on statistical probabilities.

### 🎧 Habituation Task Demo

- Show video/demo of infant habituation paradigm.
- Predict outcomes for phoneme discrimination.

### 💬 IDS Analysis

- Compare adult-directed and infant-directed speech.
- Group discussion: How does IDS support language learning?

---

## 🧠 Key Questions

1. What evidence suggests that language learning begins before birth?
2. Why are infants initially better than adults at distinguishing speech sounds?
3. How does statistical learning help infants discover word boundaries?
4. What roles do prosody and infant-directed speech play in early acquisition?

---

## 🏷️ Key Terms

| Term | Definition |
|------|------------|
| **High-Amplitude Sucking (HAS)** | A method to test infant preferences by tracking sucking rate |
| **Statistical Learning** | The ability to detect probabilities of syllable sequences |
| **Phoneme Categorization** | Classifying sounds as belonging to distinct speech categories |
| **Infant-Directed Speech (IDS)** | Speech register used with babies to facilitate learning |
| **Word Segmentation** | Identifying word boundaries in continuous speech |

---

## 📚 Reading

- Traxler (2012), Chapter 9: *Language Development in Infancy and Early Childhood* (pp. 325–344)

---

## 📝 Practice Prompt

> You are tasked with designing a study to determine whether infants can segment the word "pabu" from a stream of continuous syllables. 
> - What kind of cues would you expect them to rely on? 
> - What might you measure?

---

## 🔁 Related Weeks

- Week 11: *Metaphor &amp; Idioms*
- Week 13: *Morphological &amp; Syntactic Development*
--></description></item><item><title>Week 11: Nonliteral Language – Metaphor &amp; Idioms</title><link>https://zjpsycholin.github.io/psycholinguistics/week11/</link><pubDate>Fri, 06 Jun 2025 10:03:44 -0400</pubDate><guid>https://zjpsycholin.github.io/psycholinguistics/week11/</guid><description>&lt;p>To be published &amp;hellip;&lt;/p>
&lt;!--
## 📘 Overview

This week, we explore how people process language that **doesn’t mean exactly what it says**, including metaphors, idioms, and metonymy. We’ll review theoretical models, evidence from behavioral and neuroscience studies, and the role of **context** and **embodied cognition** in interpreting figurative expressions.

---

## 🧠 Core Concepts

### Types of Nonliteral Language

- **Metaphor**: "My lawyer is a shark."
- **Metonymy**: "The White House issued a statement."
- **Idioms**: "Kick the bucket", "Spill the beans"
- **Underspecified expressions**: Ambiguity invites inferencing.

---

### The Standard Pragmatic View

- **Literal meaning first**, then **nonliteral reinterpretation** if needed.
- Comprehension involves:
 1. Accessing literal meaning
 2. Detecting anomaly or inconsistency
 3. Reanalyzing to arrive at nonliteral interpretation
- Challenge: Empirical evidence shows **figurative meaning can be accessed directly** in context:contentReference[oaicite:0]{index=0}.

---

### The Career of Metaphor Hypothesis

- Metaphors evolve with familiarity:
 - **Novel metaphors** require comparison and mapping.
 - **Conventional metaphors** are processed more like **categorical statements**.
- E.g., "The mind is a computer" (novel) → "He’s burning the candle at both ends" (conventional):contentReference[oaicite:1]{index=1}.

---

### Embodied Cognition and Metaphor

- Understanding metaphors may involve **sensorimotor systems**.
- "Grasping the idea" may activate **grasping-related motor areas** in the brain.
- Suggests **semantic processing is grounded in perception and action** systems.

---

### Idioms and Frozen Metaphors

- Idioms have **fixed forms and meanings**, often processed as **chunks**.
- Processing depends on:
 - **Transparency** (How easily literal meaning suggests figurative meaning)
 - **Familiarity**
- Can be stored **lexically** or **constructed compositionally** depending on usage.

---

### Neural Basis of Figurative Language

- fMRI and ERP studies show:
 - **Right hemisphere** more involved in novel metaphor and joke processing.
 - **Left hemisphere** handles conventionalized expressions.
- Brain areas involved: **Inferior frontal gyrus**, **posterior temporal lobe**, **motor cortex** (for embodied metaphors):contentReference[oaicite:2]{index=2}.

---

## 📚 Reading

- Traxler (2012), Chapter 7: *Nonliteral Language Processing* (pp. 267–297)

---

## 🏷️ Key Terms

| Term | Definition |
|------|------------|
| **Metaphor** | Understanding one thing in terms of another |
| **Metonymy** | Referring to something by a related concept |
| **Idiom** | Fixed phrase with figurative meaning |
| **Embodied Cognition** | Idea that understanding is grounded in physical experience |
| **Career of Metaphor** | Development from novel to conventional metaphor usage |

---

## 🧪 In-Class Activities

### 🎭 Metaphor Generation Game

- Pairs generate metaphors for abstract concepts like **time**, **emotion**, **power**.
- Class votes on which are **conventional**, **novel**, or **mixed**.

### 🧩 Idiom Matching Task

- Students match idioms with their literal equivalents and explain **transparency**.
- Discussion: Are idioms stored or interpreted on the fly?

### 🧠 Embodiment Demonstration

- Students read sentences like “She grasped the concept” or “He kicked the idea around.”
- Reflect on physical sensations or imagery involved in interpretation.

---

## ❓ Self-Check Questions

1. How does the Standard Pragmatic View explain figurative language processing?
2. What does the Career of Metaphor hypothesis predict about metaphor familiarity?
3. How does embodied cognition challenge traditional views of language processing?
4. What brain areas are involved in metaphor and idiom comprehension?

---

## 🧩 Practice Prompt

> Sentence: “She broke the ice with a joke.” 
> - What type of nonliteral language is this?
> - Is this a conventional or novel use?
> - What mental processes are involved in understanding it?

---

## 🔁 Related Topics

- Week 10: *Dialogue and Pragmatics*
- Week 12: *Language Development – Word Learning and Meaning*

--></description></item><item><title>Week 10: 💬 Pragmatics and Dialogue I</title><link>https://zjpsycholin.github.io/psycholinguistics/week10/</link><pubDate>Tue, 03 Jun 2025 10:03:44 -0400</pubDate><guid>https://zjpsycholin.github.io/psycholinguistics/week10/</guid><description>&lt;p>To be published &amp;hellip;&lt;/p>
&lt;!--
## 📘 Overview
Everyday conversation seems effortless, but it relies on powerful **pragmatic principles** and **interactive routines**. This week introduces **Grice’s Cooperative Principle and maxims**, **conversational implicature** (including **scalar implicatures**), **speech acts** (direct vs indirect), and core structures of dialogue: **turn-taking**, **adjacency pairs**, and **repair**. We’ll also touch on **common ground** and **audience design**—how speakers tailor messages for specific listeners.

---

## 🎯 Learning Goals
By the end of Week 10, you should be able to:

- State **Grice’s Cooperative Principle** and the four **Maxims**; diagnose **violations** vs **floutings** and infer the likely **implicature**.
- Explain **conversational implicature** vs **entailment** and why implicatures are **cancelable**.
- Define **scalar implicature** (e.g., *some → not all*) and show how **context** strengthens or cancels it.
- Identify **speech acts** (locutionary/illocutionary/perlocutionary) and rewrite **direct** as **indirect** requests politely.
- Recognize **turn-taking cues**, **adjacency pairs**, and common **repair** formats in dialogue.
- Describe **common ground**/**audience design** and give one partner-specific adaptation example.

---

## 📖 Required Reading
- **Traxler (1st ed.), Chapter 8, pp. 305–315** — *Dialogue* (selected sections on pragmatics and conversation structure).

---

## 🔑 Key Concepts &amp; Mini-Explanations

### 🤝 Cooperative Principle &amp; Gricean Maxims
- **Cooperative Principle**: “Make your contribution such as is required… by the accepted purpose of the talk exchange.”
- **Maxims** 
 - **Quantity**: Be as informative as needed, no more or less. 
 - **Quality**: Do not say what you believe to be false; don’t assert without evidence. 
 - **Relation** (*Relevance*): Be relevant. 
 - **Manner**: Be clear, brief, orderly; avoid ambiguity.
- **Flouting** (overtly breaking a maxim) often **signals an implicature** (e.g., irony, understatement).

**Example** 
A: *How was the exam?* 
B: *Well, the room was very comfortable.* → Flouts **Relation**; implicates “it didn’t go well.”

---

### 🕵️ Conversational Implicature
- **Implicature**: meaning inferred from **maxim-guided reasoning**, not explicitly stated. 
- **Cancelability**: “Some students passed—**in fact, all did**.” (implicature *not all* is canceled) 
- **Entailment vs Implicature**: Entailments follow from **truth conditions** and **cannot** be canceled without contradiction.

---

### 📏 Scalar Implicature
- **Scales**: ⟨all, most, many, some⟩; ⟨always, often, sometimes⟩; ⟨must, should, may⟩. 
- Saying a **weaker** term (e.g., *some*) implicates the stronger is **not** satisfied (*not all*), **unless** context cancels it. 
- **Strengtheners**: numerals, explicit upper bounds, contrastive stress, corrective follow-ups.

**Mini-contrast** 
- *Some of the files uploaded successfully* → typically implicates *not all*. 
- *At least some of the files uploaded* → weaker commitment; implicature softened.

---

### 🎭 Speech Acts (Direct vs Indirect)
- **Locutionary** (literal form), **Illocutionary** (intended act: request, offer), **Perlocutionary** (effect on listener). 
- **Direct**: *Close the window.* 
- **Indirect**: *Could you close the window?* / *It’s a bit cold in here.* (relies on shared norms and politeness). 
- **Felicity conditions**: speaker’s authority/ability, appropriateness, sincerity.

---

### 🔁 Turn-Taking, Adjacency Pairs, and Repair
- **Turn-taking**: listeners use **TRPs** (intonation, syntax completion, gaze) to enter; **backchannels** (*uh-huh,嗯*) signal attention. 
- **Adjacency pairs**: **Q–A**, **Greeting–Greeting**, **Offer–Acceptance/Refusal**; “preferred” responses (e.g., acceptance) are quicker and simpler than dispreferred (refusals → delays, hedges). 
- **Repair**: 
 - **Self-initiated self-repair**: *I went to Bei— I mean, **Beijing**.* 
 - **Other-initiated repair**: *Sorry?* / *Who?* 
 - **Editing terms**: *uh, um, I mean, sorry, 那个* help manage trouble sources.

---

### 🧩 Common Ground &amp; Audience Design
- **Common ground**: shared knowledge/beliefs within the conversation. 
- **Audience design**: speakers adapt **detail**, **word choice**, and **reference** (e.g., *the big blue mug* vs *that one*) to a partner’s needs and prior knowledge. 
- **Partner-specific** labels can form (e.g., “the **panda cup**”); later references become shorter once established.

---

## 📝 Pre-Class Activities
1. **Read** pp. 305–315 and underline one **implicature** example and one **indirect request**. 
2. **Context test (5 min)**: Write two sentences using **some**—one where **not all** is clearly implicated, and one where it is **canceled**. 
3. **Politeness rewrite**: Turn *Give me your notes* into two indirect, polite requests.

---

## 💬 In-Class Activities

### 1) 🎭 Gricean Maxims Skits (12 min)
- **Groups of 3–4** write a 4-turn mini-dialogue **flouting** one maxim (clearly). 
- Perform; audience identifies **which maxim** and states the **implied meaning**.

### 2) 🔢 Scalar Cards (12 min)
- **Pairs** draw a **scale card** (e.g., ⟨all…some⟩). 
- One crafts a sentence with the **weaker** term; partner judges the **implicature** and tries to **cancel** it felicitously.

### 3) 🗣️ Indirect Speech-Act Clinic (10 min)
- Convert blunt requests into **two** polite versions; label the **illocutionary force** and **felicity conditions**. 
 - *Email me the slides tonight.* → *Could you…?* / *Would it be possible…?*

### 4) 🔁 Turn-Taking &amp; Repair Roleplay (12 min)
- In triads, one **tells a short story**; listener provides **backchannels** and initiates **repair** at least twice. 
- Rotate roles; tally **editing terms** and **repair types**; discuss what felt natural in English vs Chinese.

### 5) 👥 Common-Ground Referencing Game (10 min)
- Pairs describe items in a small **grid** (colors/shapes). Establish a **label** (“the snowflake star”), then try shorter references. 
- Reflect: how quickly do partner-specific labels emerge?

### 6) Wrap (3 min)
- Write one **diagnostic cue** you’ll use to spot an **implicature** in reading/listening.

---

## 🔁 Post-Class Review
- **One-pager**: For two dialogues, identify (a) maxim flouted, (b) inferred implicature, (c) whether it is **cancelable**, and (d) how **context** affects it. 
- **Reflection (100–120 words)**: When do you personally prefer **indirect** requests in English? Give one example and reasoning (politeness, face needs, hierarchy).

---

## 🏠 Homework
- **Textbook “Test Yourself”** (Ch. 8, pp. 305–315): items on maxims, implicature, and speech acts. 
- **Short analysis (≈150–200 words)**: Record or recall a brief conversation (English or Chinese). Transcribe 6–8 turns; annotate **adjacency pairs**, **backchannels**, and **repair**. Point out any implicatures.

---

## 🧩 Self-Check Questions

**Q1.** How does **implicature** differ from **entailment**? 
&lt;!-- Implicatures are inference-based, context-dependent, and cancelable; entailments follow from sentence meaning and cannot be canceled without contradiction. -->
&lt;!--
**Q2.** Name the four **Gricean Maxims** and give a one-phrase description. -->
&lt;!-- Quantity (informative), Quality (truthful/evidenced), Relation (relevant), Manner (clear/brief/orderly). -->
&lt;!--
**Q3.** What is a **scalar implicature**? Give an example with **some**. -->
&lt;!-- Using a weaker scalar term (e.g., “some”) typically implicates that a stronger alternative (“all”) does not hold—unless context cancels it. -->
&lt;!--
**Q4.** Give one reason to use an **indirect request** instead of a direct one. -->
&lt;!-- Politeness and face management, reducing imposition, maintaining social harmony or hierarchy. -->
&lt;!--
**Q5.** What’s the preferred pattern in **repair** and why? -->
&lt;!-- Self-initiated self-repair is preferred: it is least disruptive and shows speaker responsibility for fixing trouble. -->
&lt;hr>
&lt;!--
## 🧰 Key Terms
**Cooperative Principle**, **Gricean Maxims (Quantity/Quality/Relation/Manner)**, **Conversational implicature**, **Scalar implicature**, **Cancelability**, **Speech act** (locutionary/illocutionary/perlocutionary), **Indirect request**, **Turn-taking**, **TRP**, **Adjacency pair**, **Preferred/dispreferred response**, **Repair**, **Backchannel**, **Common ground**, **Audience design**.

---

## 🌐 Optional Resources
- Short explainers on **Grice’s maxims** and **implicature** (intro videos/articles). 
- Politeness strategies in English vs Chinese (brief guides). 
- Conversation analysis snippets for **adjacency pairs** and **repair**.

---

### ✅ How to use these notes
- **Before class:** bring two implicature examples and your **scalar** sentences. 
- **During class:** label the **maxim** and state the **inference** explicitly. 
- **After class:** annotate one real conversation for **pairs**, **backchannels**, **repairs**, and **implicatures**.

-->
&lt;!--
## 📘 Overview

This week we explore **language use in social contexts**, focusing on how meaning arises through interaction. Unlike monologues or isolated utterances, **dialogue** involves cooperation, shared knowledge, and continuous adjustment between interlocutors. We’ll cover foundational principles such as **Grice’s maxims**, as well as more dynamic notions like **common ground**, **audience design**, and **perspective-taking** in real-time conversation.

---

## 🧠 Core Concepts

### Gricean Maxims of Cooperative Communication

According to philosopher H.P. Grice, speakers follow four maxims when they aim to be cooperative:

1. **Quantity** – Be as informative as necessary; no more, no less.
2. **Quality** – Be truthful; do not say what you believe is false.
3. **Relation** – Be relevant.
4. **Manner** – Be clear; avoid ambiguity and obscurity.

> 🧠 Violations of these maxims often trigger **implicatures**—indirect meanings derived by listeners.

---

### Dialogue Is Interactive

- Dialogue is not just **taking turns** but **jointly constructing meaning**.
- Speakers and listeners **coordinate their actions**, **adjust timing**, and **anticipate responses**.
- The presence of **feedback** (e.g., head nods, “uh-huh”) plays a crucial role in this alignment.

---

### Common Ground

- **Common ground** = shared knowledge between speakers.
- Speakers tailor their messages based on **assumptions about what the listener knows**.
- This helps reduce ambiguity and processing load.

> Example: 
> “You know, the professor from last semester” presumes shared knowledge.

---

### Audience Design

- Speakers often adjust **length**, **detail**, and **clarity** depending on their audience.
- Related to **Theory of Mind**—our ability to model others’ mental states.
- Studies show that speakers consider their listener’s perspective, but **not always accurately or efficiently**.

---

### Egocentric vs. Listener-Oriented Production

- **Egocentric production**: Speakers sometimes overestimate the listener’s knowledge.
- **Listener-oriented production**: Adjustments based on assumptions about listener's needs.

> Real-life examples: Directions to a local vs. a tourist; inside jokes among friends.

---

### Comprehension and Perspective-Taking

- Listeners also work to **adopt the speaker’s viewpoint**.
- Successful comprehension requires **active inferencing** and **constant updating** of mental models.

---

## 📚 Reading

- Traxler (2012), Chapter 8: *Dialogue* (pp. 305–321)

---

## 🏷️ Key Terms

| Term | Definition |
|------|------------|
| **Pragmatics** | The study of how meaning is conveyed in context |
| **Gricean Maxims** | Principles speakers follow for cooperative communication |
| **Common Ground** | Shared knowledge and assumptions in conversation |
| **Audience Design** | Adjusting speech based on listener’s needs |
| **Egocentric Production** | When speakers fail to account for listener’s perspective |

---

## 🧪 In-Class Activities

### 🗣️ Gricean Maxim Skits

- Students work in groups to create **short dialogues** where one speaker **violates a maxim**.
- Class guesses which maxim is violated and why.

### 🤝 Common Ground Game

- One student describes an object to another while **avoiding direct naming**.
- Pairs compare strategies and discuss what knowledge was assumed.

### 🔁 Audience Shift Exercise

- Students rewrite a short explanation (e.g., “How to play chess”) for:
 - a 6-year-old
 - a peer
 - a grandparent
- Class reflects on how the explanation changed and why.

---

## ❓ Self-Check Questions

1. What are the four Gricean maxims, and how do they contribute to effective communication?
2. How does common ground influence what we say and how we interpret what others say?
3. What is audience design, and how is it related to Theory of Mind?
4. What evidence shows that dialogue is a cooperative, interactive process?

---

## 🧩 Practice Prompt

> Dialogue: 
> A: “Where’s the salad dressing?” 
> B: “There’s a gas station around the corner.” 
>
> - What maxim is being violated here?
> - What implicature might be inferred?

---

## 🔁 Related Chapters

- Chapter 7: *Non-Literal Language* (for implicatures and metaphor)
- Chapter 9: *Language Development* (development of perspective-taking)

--></description></item><item><title>Week 9: 🎭 Metaphor &amp; Idioms</title><link>https://zjpsycholin.github.io/psycholinguistics/week09/</link><pubDate>Fri, 30 May 2025 10:03:44 -0400</pubDate><guid>https://zjpsycholin.github.io/psycholinguistics/week09/</guid><description>&lt;p>To be published &amp;hellip;&lt;/p>
&lt;!--

## 📘 Overview
Not all meaning is literal. We routinely comprehend **metaphors** (“Ideas are seeds”) and **idioms** (“spill the beans”) quickly and effortlessly. This week explores **how**: Do we **always** analyze literal meaning first, or can figurative meanings be accessed **directly**? When do **context**, **familiarity**, and **transparency** help? You’ll generate metaphors, match idioms to meanings, compare English idioms with Chinese 成语, and practice explaining how different theories account for your observations.

---

## 🎯 Learning Goals
By the end of Week 9, you should be able to:

- Explain key theories of **metaphor** (Standard Pragmatic Model, Direct Access, Graded Salience, Structure Mapping, Conceptual Metaphor).
- Describe major **idiom processing** accounts (Lexicalized/Direct Retrieval, Configuration/Construction, Decomposability).
- Predict how **familiarity**, **transparency**, and **context** influence processing time and interpretive preference.
- Apply these ideas to analyze **English–Chinese** idiom pairs and paraphrase figurative meanings accurately.
- Relate basic **psycholinguistic evidence** (e.g., priming, predictability, N400) to the processing of nonliteral meaning.

---

## 📖 Required Reading
- **Traxler (1st ed.), Chapter 7, pp. 267–285** — *Nonliteral Language Processing* (focus on metaphor &amp; idioms).

---

## 🔑 Key Concepts &amp; Mini-Explanations

### 🧭 Two Big Questions
1) **Access Order**: Do listeners compute a **literal** meaning first and then revise (Standard Pragmatic Model), or can figurative meanings be **directly accessed** when salient/expected (Direct Access; Graded Salience)? 
2) **Representation**: Are idioms stored as **whole lexical entries** (like long words) or **constructed** compositionally using syntax/semantics when transparent?

### 🧩 Metaphor Processing — Competing Views
- **Standard Pragmatic Model (SPM)**: literal meaning is computed first; if it fails, infer figurative meaning via **Gricean** reasoning. Predicts early **literal preference** unless context strongly biases figurative reading. 
- **Direct Access View**: figurative meaning can be retrieved **without** a literal-first step when it is conventional/salient or contextually primed. 
- **Graded Salience Hypothesis (Giora)**: the **most salient** meaning (familiar, frequent, conventional) is accessed **first**—whether literal **or** figurative. Thus conventional metaphors (e.g., “grasp an idea”) can be as fast as literal uses. 
- **Structure Mapping (Gentner)**: metaphor = **analogy**; align **relational structure** from source (CONCRETE) to target (ABSTRACT), preserving **relations** more than surface features. 
- **Conceptual Metaphor Theory (Lakoff &amp; Johnson)**: systematic mappings in cognition (e.g., **TIME IS MONEY**, **ARGUMENT IS WAR**) organize many expressions, guiding interpretation and expectation.

**Processing signatures (typical findings):** 
- **Context** and **conventionality** reduce difficulty; familiar metaphors often show **fast reading** and **reduced N400** (predictability). 
- **Novel** metaphors cost more: extra integration and imagery can increase processing time.

### 🧠 Idioms — Storage, Construction, and Cues
- **Direct Retrieval / Lexicalized Idioms**: frequent, fixed idioms stored as **single entries**; when context strongly predicts the idiom, readers show **faster** processing (an “idiom superiority” effect). 
- **Configuration / Construction**: readers recognize an emerging **idiom configuration** across words; once enough cues accumulate, the figurative template is activated. 
- **Decomposability**: some idioms are **transparent** (each part contributes; *spill the beans* ≈ *reveal a secret*), others are **opaque** (*kick the bucket*). Decomposable idioms are more flexible (passivization, modification) and allow partial **semantic priming** from constituents. 
- **Familiarity &amp; Transparency**: high **familiarity** speeds access; **transparency** supports partial compositionality and robustness across syntactic variations. 
- **Literal Plausibility**: when a literal reading remains plausible and context is weak, comprehenders may experience competition/slowdowns until context settles meaning.

### 🔬 Evidence Snapshots (beginner-friendly)
- **Priming/Predictability**: Strongly biasing contexts lead to **earlier** access of figurative meanings and faster completions. 
- **ERP (N400)**: predictable idioms/metaphors often show **smaller N400** at key words; anomalous or novel figurative uses show **larger N400** (greater integration cost). 
- **Eye-tracking**: shorter first-pass times on idiomatic regions when idiom completion is **expected**; longer regressions for **novel/opaque** metaphors.

---

## 📝 Pre-Class Activities
1. **Read** pp. 267–285 and mark **two metaphors** (one conventional, one novel) and **two idioms** (one transparent, one opaque). 
2. **Context Builder**: Write **two short contexts** that make *spill the beans* either (a) highly predictable or (b) very unlikely. 
3. **Cross-lingual List**: Bring **3 English idioms** and **3 Chinese 成语/惯用语** that you like; note **familiarity** and **transparency** (1–5 scale).

---

## 💬 In-Class Activities

### 1) 🎨 Metaphor Generation (12 min)
- **Pairs**: Generate **3 metaphors** for “learning” using different sources (LEARNING IS **travel**, **construction**, **growth**). 
- **Share**: Identify which view (Structure Mapping vs Conceptual Metaphor) best explains each.

### 2) 🧩 Idiom Matching (15 min)
- **Teams**: Match **idioms → meanings** and rate **familiarity** (1–5) and **transparency** (1–5). 
- Predict which items should be **fastest** in comprehension and which should be **slowest**, and why.

**Sample set** 
- spill the beans — reveal a secret 
- bite the bullet — endure something difficult 
- hit the sack — go to sleep 
- kick the bucket — die 
- break the ice — start social interaction 
- cut corners — do something cheaply/poorly

### 3) 🔀 Literal vs Figurative Competition (10 min)
- Instructor presents **literal-plausible** contexts (e.g., a chef literally **spills beans**). 
- **Discuss**: Where did you hesitate? Which theory predicts the conflict and how would **context** resolve it?

### 4) 🧪 Mini Priming Demo (10 min)
- Rapid sentences that **strongly** predict an idiom vs **neutral** contexts; students indicate “fast/slow feel.” 
- Relate to **predictability** and **N400** intuition.

### 5) 🌏 English–Chinese Idiom Clinic (10 min)
- Small groups compare English idioms with **Chinese** equivalents (literal images, transparency, allowed variations). 
- Note where **direct translation fails** and how **conceptual mappings** differ.

### 6) Quick Wrap (3 min)
- Write one **rule of thumb** you’ll use to handle figurative language in reading/listening.

---

## 🔁 Post-Class Review
- **One-pager**: Choose **one metaphor** and **one idiom** from class and explain their processing using **two theories** each. 
- **Reflection (100–120 words)**: When do you personally **default to literal** vs **figurative**? Give one example from your L2 experience.

---

## 🏠 Homework
- **Textbook “Test Yourself”** (Ch. 7, pp. 267–285) items on metaphor and idioms. 
- **Short write-up (≈150–200 words)**: Compare **transparent vs opaque** idioms you encountered; predict effects of **familiarity** and **context** on speed/accuracy. 
- **Optional**: Build a tiny **idiom deck** (8 cards): idiom, meaning, context sentence, transparency 1–5.

---

## 🧩 Self-Check Questions

**Q1.** What does the **Graded Salience Hypothesis** predict about conventional metaphors? 
&lt;!-- The most salient meaning—often the conventional figurative one—will be accessed first, so familiar metaphors can be processed as quickly as literal language. -->
&lt;!--
**Q2.** Give one prediction that distinguishes **SPM** from **Direct Access** in early processing. -->
&lt;!-- SPM predicts an initial literal preference unless context forces figurative; Direct Access allows immediate figurative access when figurative meaning is highly familiar/salient. -->
&lt;!--
**Q3.** What is **idiom decomposability**, and why does it matter? -->
&lt;!-- Decomposability reflects how much constituents contribute to figurative meaning; decomposable idioms are more flexible and can show constituent-based priming and easier syntactic variation. -->
&lt;!--
**Q4.** How do **familiarity** and **transparency** typically affect idiom processing? --> 
&lt;!-- Higher familiarity speeds access; greater transparency supports partial compositional processing and robust comprehension across contexts. -->
&lt;!--
**Q5.** What ERP pattern is often observed for **predictable idiomatic completions**? -->
&lt;!-- Reduced N400 relative to unpredictable or anomalous completions, reflecting easier semantic integration. -->
&lt;hr>
&lt;!--
## 🧰 Key Terms
**Metaphor**, **Standard Pragmatic Model**, **Direct Access**, **Graded Salience Hypothesis**, **Structure Mapping**, **Conceptual Metaphor**, **Idiom**, **Decomposability (transparent/opaque)**, **Familiarity**, **Transparency**, **Literal plausibility**, **Predictability**, **N400**.

---

## 🌐 Optional Resources
- Short explainers on **metaphor vs analogy** and **idiom decomposability** (intro videos/articles). 
- Bilingual lists of English–Chinese idioms for cross-cultural comparison. 
- Tools for building **flashcards** to practice idioms with context.

---

### ✅ How to use these notes
- **Before class:** bring your idiom/metaphor lists and contexts. 
- **During class:** always state which **theory** you’re using and what **cue** (familiarity, transparency, context) supports your prediction. 
- **After class:** test yourself by paraphrasing figurative meaning in **plain literal** language.

-->
&lt;!--
## 📘 Overview

This week introduces one of the most fundamental processes in discourse comprehension: resolving **reference**. Whether it's identifying who “she” refers to or which object “that” indicates, listeners and readers must rapidly and accurately map **anaphoric expressions** to the correct **referents**. We'll examine **linguistic cues**, **working memory**, and theoretical models like **Binding Theory**, **Centering Theory**, and the **Informational Load Hypothesis**.

---

## 🧠 Core Concepts

### What Is Reference?

- **Referents** are entities introduced or assumed in discourse.
- **Anaphora** refers to expressions (e.g., pronouns) that depend on a previously introduced referent.

---

### Characteristics That Aid Co-Reference

#### Of Referents:

| Factor | Effect |
|--------|--------|
| **Recency** | Recently mentioned referents are more accessible. |
| **Grammatical role** | Subjects are more likely referents. |
| **First mention** | Entities introduced early in a sentence are favored. |
| **Discourse focus** | Salient or topical entities are more accessible:contentReference[oaicite:0]{index=0}.

#### Of Anaphors:

| Feature | Effect |
|---------|--------|
| **Definiteness** | “The man” vs. “a man” guides expectations. |
| **Syntactic position** | Embedded vs. main-clause anaphors vary in processing ease. |
| **Ambiguity** | Ambiguous pronouns (e.g., “he” in male-dense contexts) increase difficulty:contentReference[oaicite:1]{index=1}.

---

### Linguistic Theories of Reference

#### Binding Theory

- A syntactic theory from generative grammar:
 - **Principle A**: Reflexives must refer to a subject in the same clause.
 - **Principle B**: Pronouns cannot refer to a subject in the same clause.
 - **Principle C**: Names cannot be coreferential with a pronoun that precedes them.

#### Psycholinguistic Models

| Model | Summary |
|-------|---------|
| **Memory Focus Model** | Refers to the accessibility of referents held in working memory. |
| **Centering Theory** | Suggests that discourse coherence is maintained by linking current utterances to salient prior “centers.” |
| **Informational Load Hypothesis** | Proposes that pronoun interpretation is easiest when memory demands are low and discourse is simple:contentReference[oaicite:2]{index=2}.

---

## 📚 Reading

- Traxler (2012), Chapter 6: *Reference* (pp. 241–260)

---

## 🏷️ Key Terms

| Term | Definition |
|------|------------|
| **Reference** | The act of pointing to or identifying an entity in discourse |
| **Anaphor** | A word (like “she” or “it”) that refers back to a previously mentioned item |
| **Co-reference** | The relationship between a referring expression and its antecedent |
| **Binding Theory** | A theory of syntactic constraints on reference |
| **Centering Theory** | A discourse model that explains focus and referent selection based on coherence |
| **Informational Load Hypothesis** | A theory explaining pronoun processing difficulty based on memory and sentence complexity |

---

## 🧪 Examples &amp; In-Class Activities

### 🔍 Pronoun Puzzle Game

- Show short sentences with multiple characters (e.g., “John hugged Mike. He smiled.”)
- Students must infer who “he” refers to and explain their reasoning.

### 📖 Binding Theory Test

- Provide sentence pairs and ask whether the reference is acceptable:
 - “Mary saw herself in the mirror.” ✅
 - “Herself saw Mary in the mirror.” ❌
- Discuss each Binding Principle with examples.

### 🧠 Co-reference Matching Task

- Present mini-stories with pronouns and noun phrases.
- Students draw arrows or diagrams linking referents.

---

## ❓ Self-Check Questions

1. What kinds of referents are easiest to recall in discourse?
2. What factors influence how we interpret ambiguous pronouns?
3. What are the three principles of Binding Theory?
4. How does Centering Theory explain discourse coherence?
5. How does sentence complexity affect anaphora resolution?

---

## 🧩 Practice Prompt (Adapted)

> Sentence: “Laura told Jenny that she had won the award.”
> - Who could “she” refer to?
> - What contextual or structural information might disambiguate the reference?

---

## 🔁 Related Chapters

- Chapter 5: *Discourse Processing* (situation models and coherence)
- Chapter 7: *Non-Literal Language Processing* (use of pronouns in metaphoric contexts)
--></description></item><item><title>Week 8: 🧩 Sentence Processing II</title><link>https://zjpsycholin.github.io/psycholinguistics/week08/</link><pubDate>Tue, 27 May 2025 10:03:44 -0400</pubDate><guid>https://zjpsycholin.github.io/psycholinguistics/week08/</guid><description>&lt;p>To be published &amp;hellip;&lt;/p>
&lt;!--
## 📘 Overview
Building sentence structure in real time means juggling **multiple possibilities** while keeping prior words active in **working memory**. This week focuses on **how ambiguities get resolved** under **memory constraints**. We’ll examine **cue-based retrieval and interference**, **locality effects**, and **expectation-based (surprisal)** accounts. You’ll practice **PP-attachment** and **relative clause** parsing, run a **memory–interference mini-demo**, and learn to read **eye-tracking metrics** used in sentence processing research.

---

## 🎯 Learning Goals
By the end of Week 8, you should be able to:

- Explain how **working memory** and **cue-based retrieval** affect online parsing.
- Identify **similarity-based interference** and **locality** effects in comprehension.
- Apply **expectation/surprisal** logic to predict harder vs. easier continuations.
- Diagnose **PP-attachment** and **relative clause** ambiguities and justify your analysis.
- Interpret basic **eye-tracking measures** (first pass, regressions, total time) as evidence for ambiguity resolution and reanalysis.

---

## 📖 Required Reading
- **Traxler (1st ed.), Chapter 4, pp. 155–181** — *Sentence Processing* (ambiguity resolution, memory constraints, attachment, and individual differences).

---

## 🔑 Key Concepts &amp; Mini-Explanations

### 🧠 Working Memory in Parsing
- **Maintenance + retrieval**: Words/phrases must be **kept active** and later **retrieved** to integrate new input. Load rises with **multiple clauses**, **embeddings**, or **long dependencies**.
- **Cue-based retrieval**: The parser uses **cues** (number, gender, thematic role) to fetch the right item from memory. Retrieval is **content-addressable** (jump directly to matches), not serial search.

### 🧲 Similarity-Based Interference
- **Interference** occurs when **distractors** share cues with the true target. 
 - *Example*: *The **keys** to the cabinet **were**…* vs. *The **key** to the cabinets **were**…* (number-marked distractor can mislead agreement or slow parsing).
- Predicts **more errors/slowdowns** when **multiple nouns** match retrieval cues (e.g., two plural NPs near the verb).

### 📏 Locality &amp; Dependency Length
- **Locality effects**: Integrating two elements is harder when they are **farther apart** (longer **dependency length**). 
- Nested/center-embedded clauses produce **processing overload**: 
 - *The reporter [that the senator [that the journalist criticized] attacked] resigned.*

### 📈 Expectation &amp; Surprisal
- **Expectation-based parsing**: Readers build **probabilistic expectations** from **frequency** and **context**. 
- **Surprisal** ≈ –log P(word | context): **unexpected** words/structures cause larger slowdowns. 
- Predicts easier processing when context **strongly predicts** the upcoming structure.

### 🧩 PP-Attachment Ambiguity
- *I saw the man with the telescope.* 
 - **VP-attachment**: *saw (with the telescope)* (instrument). 
 - **NP-attachment**: *the man (with the telescope)* (modifier). 
- Resolution relies on **verb bias**, **plausibility**, and **prosody/punctuation**.

### 🧱 Relative Clause Ambiguity (Subject vs. Object RC)
- **Subject RC (SRC)**: *The reporter [who __ attacked the senator] …* 
- **Object RC (ORC)**: *The reporter [who the senator attacked __ ] …* 
- ORCs often **harder** (longer dependency, interference from interveners). **Animacy** and **case marking** can mitigate difficulty (cross-linguistic variation).

### 👣 Evidence from Eye-Tracking
- **First fixation** / **first pass** time: early processing. 
- **Regression path (go-past)**: time including the **first regression**—sensitive to **garden-paths**. 
- **Total time**: later integration/repair. 
- **Where slowdowns appear** helps localize **disambiguation points** and **reanalysis** cost.

### 🧪 Good-Enough Comprehension
- Under load/time pressure, readers may build **shallow** representations guided by plausibility (“good enough”) rather than full syntactic detail—especially in noisy contexts.

---

## 📝 Pre-Class Activities
1. **Read** pp. 155–181 and mark one **PP-attachment** and one **RC** example that slowed you down. 
2. **Cloze guess**: For the stem *“The editor realized the article …”* write **three continuations** (from most to least expected). 
3. **Memory rehearsal**: Read and repeat aloud a sentence with one **center-embedding**; notice where you hesitate.

---

## 💬 In-Class Activities

### 1) Interference Mini-Demo (10 min)
- Instructor presents pairs; you predict which is harder and why: 
 1. *The **keys** to the cabinet **were** on the table.* 
 2. *The **key** to the cabinets **were** on the table.* 
- Discuss: Which cues (number, proximity) created **retrieval conflict**?

### 2) PP-Attachment Workshop (15 min)
- **Pairs** label each sentence as **VP-attach** or **NP-attach** and give **one cue** (verb bias/plausibility/prosody) that favors your choice. 
 - *I photographed the girl with the **drone**.* 
 - *I bumped the vase with the **elbow**.* 
 - *They spotted the hiker with the **binoculars**.* 
- **Bonus rewrite**: Add **comma**/**that**/**prosody** to force your intended reading.

### 3) Relative Clause Lab (15 min)
- **Triads**: Classify as **SRC** or **ORC**; predict difficulty and **why** (distance/interference/animacy). 
 - *The researcher who __ praised the technicians presented the results.* 
 - *The researcher who the technicians praised __ presented the results.* 
 - *The patient that the nurse with the badges comforted __ recovered quickly.* 
- **Tweak** animacy/case to reduce difficulty and re-test your prediction.

### 4) Surprisal &amp; Continuations (10 min)
- Groups propose **high- vs low-expectation** continuations for stems like: 
 - *Because the chef forgot the* … 
 - *When the witness identified the* … 
- Rank continuations from most to least **expected**; predict where **slowdowns** would occur.

### 5) Reading Measures Quick Read (8 min)
- Given a short ambiguous paragraph, identify the **disambiguation region**. 
- Decide which measure (first pass, go-past, total time) would likely show the **largest effect**, and why.

### 6) Wrap (2 min)
- On a sticky: write **one interference cue** and **one way** to reduce it (e.g., change number/animacy or add a function word).

---

## 🔁 Post-Class Review
- **One-pager**: For one PP sentence and one RC sentence, list: ambiguity type → cues considered → final parse → predicted **slowdown region**. 
- **Reflection (100–120 words)**: Which framework (**locality**, **interference**, or **surprisal**) best explained your own slowdowns today? Give one concrete example.

---

## 🏠 Homework
- **Textbook “Test Yourself”** (Ch. 4, pp. 155–181) items on ambiguity resolution and memory constraints. 
- **Short analysis (≈150–200 words)**: Choose an English news sentence with a PP or RC ambiguity. Analyze resolution using **two** lenses (e.g., interference **and** surprisal). 
- **Optional**: Collect two **self-made minimal pairs** that manipulate **distance** (short vs. long dependency) and predict which will be harder.

---

## 🧩 Self-Check Questions

**Q1.** What is **cue-based retrieval**, and how does it explain interference? 
&lt;!-- Retrieval uses diagnostic cues (e.g., number, case) to directly access items in memory; similar distractors that partially match the cues compete, causing slowdowns or errors. -->
&lt;!--
**Q2.** Give an example of a **locality effect**. -->
&lt;!-- Longer distances between dependent elements (e.g., subject–verb) increase processing cost, especially in center-embedded structures. -->
&lt;!--
**Q3.** How does **surprisal** predict processing difficulty? -->
&lt;!-- Higher surprisal (lower conditional probability of a word/structure) leads to larger processing slowdowns at that point. -->
&lt;!--
**Q4.** In *I saw the man with the telescope*, what cues could bias **VP** vs **NP** attachment? -->
&lt;!-- Verb instrument bias (see/photograph), plausibility of instrument vs modifier, prosody/commas, and discourse focus. -->
&lt;!--
**Q5.** Why are **object relative clauses** often harder than **subject relatives**? --> 
&lt;!-- Longer dependencies and more opportunities for similarity-based interference due to intervening nouns; animacy/case cues can mitigate the cost. -->
&lt;hr>
&lt;!--
## 🧰 Key Terms
**Cue-based retrieval**, **Similarity-based interference**, **Locality (dependency length)**, **Expectation**, **Surprisal**, **PP-attachment (VP vs NP)**, **Subject/Object relative clause (SRC/ORC)**, **Filler–gap dependency**, **First pass/Go-past/Total time**, **Good-enough processing**.

---

## 🌐 Optional Resources
- Short explainers on **cue-based parsing** and **surprisal** (intro-level blog posts/videos). 
- Eye-tracking visualization examples showing **regressions** at disambiguation points. 
- Practice sets for **PP-attachment** and **relative clause** ambiguities.

---

### ✅ How to use these notes
- **Before class:** preview key terms; try the cloze/continuation predictions. 
- **During class:** justify each parse with **explicit cues** (bias, plausibility, distance). 
- **After class:** connect your slowdowns to **interference**, **locality**, or **surprisal** and note which lens felt most explanatory.

-->
&lt;!--
## 📘 Overview

This week continues our exploration of discourse processing, with a special focus on how we use **inference** to connect and enrich the information conveyed across sentences. We explore key types of inferences—**bridging**, **elaborative**, and **predictive**—and the cognitive and neural mechanisms that support their generation during reading and listening.

---

## 🧠 Core Concepts

### What Are Inferences?

- Inferences are **unstated connections or conclusions** drawn by the reader to construct a **coherent mental representation**.
- Readers and listeners go beyond literal input to **fill in gaps**, **explain causal links**, and **anticipate outcomes**.

---

### Bridging Inferences (Backward)

- Needed to **maintain coherence** when connections are implicit.
- Example:
 > “The vase fell. Sarah swept up the pieces.”
 - Inference: The vase broke.
- These are **memory-based** and often automatic.

---

### Elaborative Inferences (Forward)

- Add **new, plausible information** beyond what’s stated.
- Example:
 > “The actress stepped onto the stage.” → She might perform or speak.
- These depend on **goals**, **schemas**, and **world knowledge**.
- Can be **optional** and variable across readers.

---

### Predictive Inferences

- Readers often anticipate **what will come next**.
- Example:
 > “The storm clouds gathered...” → Rain is likely.
- The **strength** of predictive inferences depends on context and individual differences.

---

### Memory and Updating

- **Situation models** are updated as new information arrives.
- Inferences can guide **activation**, **integration**, and **suppression** processes.
- Updating involves deciding whether new input **modifies**, **adds to**, or **replaces** previous info.

---

### Brain and Inference Generation

- Neuroimaging studies show:
 - **Medial prefrontal cortex** supports situation updating.
 - **Right hemisphere** often recruited for making global coherence inferences.
- Patients with brain injury often struggle with **bridging and elaborative** inference tasks:contentReference[oaicite:0]{index=0}.

---

## 📚 Reading

- Traxler (2012), Chapter 5: *Discourse Processing* (pp. 210–230)

---

## 🏷️ Key Terms

| Term | Definition |
|------|------------|
| **Inference** | A mental process of deriving unstated conclusions or connections |
| **Bridging Inference** | Inference that connects a new sentence to previous discourse |
| **Elaborative Inference** | Inference that enriches the situation model with unstated but plausible information |
| **Predictive Inference** | Expectation about upcoming content based on prior context |
| **Situation Model Updating** | The revision of mental representations as discourse unfolds |

---

## 🧪 Examples &amp; In-Class Activities

### 🔄 Inference Generation Challenge

- Provide students with short texts missing an explicit link (e.g., cause/effect).
- Ask: “What must be true for this story to make sense?”

### 🧠 Bridging vs. Elaborative Sorting Task

- Given 10 example inferences, students classify them as **bridging**, **elaborative**, or **predictive**.

### 📖 Contextual Prediction Game

- Pause after a context sentence.
- Students write down what they think comes next.
- Compare against actual continuation and reflect.

---

## ❓ Self-Check Questions

1. What is the difference between bridging and elaborative inferences?
2. How do situation models change as we receive new input?
3. What kinds of information support predictive inferences?
4. How does the brain contribute to inference generation?
5. Can all inferences be controlled? Are they always useful?

---

## 🧩 Practice Prompt (Adapted)

> Read the sentence: 
> “The engine sputtered, and the car rolled to a stop.” 
> - What inferences can you draw about what happened? 
> - What types of inferences are these, and how do they support comprehension?

---

## 🔁 Related Chapters

- Chapter 4: *Sentence Processing* (syntactic ambiguity and interpretation)
- Chapter 6: *Reference* (linking entities across discourse)
--></description></item><item><title>Week 7: 🧩 Sentence Processing I</title><link>https://zjpsycholin.github.io/psycholinguistics/week07/</link><pubDate>Sat, 24 May 2025 10:03:44 -0400</pubDate><guid>https://zjpsycholin.github.io/psycholinguistics/week07/</guid><description>&lt;p>To be published &amp;hellip;&lt;/p>
&lt;!--
## 📘 Overview
When we read or listen, we **don’t** wait for a sentence to end—we build structure **incrementally**, word by word. This can lead to **temporary ambiguity** and “garden-path” experiences where our first guess turns out wrong and we must **reanalyse**. This week introduces **core parsing strategies** (e.g., **Minimal Attachment**, **Late Closure**), classic garden-path examples, and the basic contrast between **syntax-first** and **constraint-based** views of comprehension.

---

## 🎯 Learning Goals
By the end of Week 7, you should be able to:

- Explain **incremental parsing** and why temporary ambiguity is common.
- Identify and diagnose **garden-path sentences** and describe **reanalysis**.
- Describe **Minimal Attachment** and **Late Closure** and apply them to examples.
- Contrast **syntax-first** vs **constraint-based/interactive** accounts at a basic level.
- Use **plausibility** and **prosody** intuitions to predict easier vs harder readings.

---

## 📖 Required Reading
- **Traxler (1st ed.), Chapter 4, pp. 141–154** — *Sentence Processing* (introduction, ambiguity, garden paths, core strategies).

---

## 🔑 Key Concepts &amp; Mini-Explanations

### ⏱️ Incremental Parsing &amp; Temporary Ambiguity
- **Incremental**: The parser commits to a structure as each word arrives.
- **Temporary ambiguity**: Early words allow **multiple** structures. Later words can **disconfirm** the initial choice → **garden-path**.

### 🌿 Garden-Path Effect &amp; Reanalysis
- **Garden-path sentence**: lures the parser into a wrong analysis, forcing a **backtrack**.
 - *Example*: “While Anna dressed **the baby** played in the crib.” 
 Initial parse treats **the baby** as object of **dressed**; later **played** forces reanalysis (the baby = subject of played).
- **Reanalysis cost**: measurable slowdowns (longer reading times, regressions in eye-tracking).

### 🧭 Two Heuristics: Minimal Attachment &amp; Late Closure
- **Minimal Attachment (MA)**: Prefer the structure with **fewer new nodes** (simplest tree). 
- **Late Closure (LC)**: Attach incoming material to the **phrase currently being processed** (keep it in the current clause/VP) whenever grammatically possible.
- These heuristics predict classic garden-paths such as reduced relatives:
 - *“The horse **raced past the barn** fell.”* 
 MA/LC bias “raced” as main-verb; later “fell” forces reanalysis to **reduced relative**: *The horse [that was raced past the barn] fell.*

### 🧱 Syntax-First vs Constraint-Based (Big Picture)
- **Syntax-first**: An initial, **purely structural** parse (driven by MA/LC); semantics/pragmatics integrate **after** a first pass.
- **Constraint-based (interactive)**: Multiple interpretations compete; **syntax, semantics, frequency, plausibility, prosody** all contribute **from the start**. Strong context can **prevent** some garden-paths.

### 🧪 Cues that Modulate Difficulty
- **Verb bias/subcategorization** (expectation for certain complements). 
- **Plausibility** (world knowledge can discourage unlikely parses). 
- **Prosody/Punctuation** (comma intonation can reduce garden-paths in speech/writing). 
- **Morphology/Function words** (that/which, case markers, aspect cues) guide attachment decisions.

---

## 📝 Pre-Class Activities
1. **Read** pp. 141–154 and flag any sentence that made you **slow down**. 
2. **Diagnose 3 sentences**: For each, write your **initial parse** and the **final correct parse** (one can be from the textbook). 
3. **Prosody experiment**: Read aloud “While the man hunted the deer ran into the woods” twice—once with a pause after **hunted**. Which felt clearer?

---

## 💬 In-Class Activities

### 1) Garden-Path Game (12 min)
- **Individually** read a list of 8 sentences. Mark **where** you felt confusion. 
- **Pairs**: decide **why** (MA? LC? Verb bias?). 
- Share 2 cases with the class.

**Sample set**
1. While the man hunted the deer ran into the woods. 
2. The old train the young. 
3. The horse raced past the barn fell. 
4. When Fred eats food gets thrown. 
5. Since Jay always jogs a mile seems easy. 
6. The raft floated down the river sank.

### 2) Strategy Lab: MA vs LC (15 min)
- **Triads**: Label each attachment decision (**Minimal Attachment** or **Late Closure**). 
- Rewrite 2 sentences to **disarm** the garden-path using commas, **that**, or verb changes. 
 - e.g., *While the man hunted, the deer ran…* / *The horse that was raced…*

### 3) Plausibility vs Structure (12 min)
- Consider pairs where structure is similar but **plausibility** differs: 
 - a) *The evidence examined by the lawyer was compelling.* 
 - b) *The prisoner examined by the lawyer was compelling.* 
- **Discuss**: Which one invites a main-verb parse for **examined**? Why does world knowledge matter?

### 4) Mini Prosody Workshop (8 min)
- Read ambiguous items **with** and **without** a pause/comma. 
- **Note**: Did a prosodic boundary help you pick the intended structure?

### 5) Quick Wrap (3 min)
- On a sticky: write **one heuristic** (MA/LC) and **one place** it can mislead.

---

## 🔁 Post-Class Review
- **One-pager**: For **two** garden-paths, chart: initial parse → disconfirming word → correct parse → which heuristic misled you. 
- **Reflection (100–120 words)**: Do your L1 (Chinese) cues (particles, word order, aspect markers) help you **avoid** certain English garden-paths?

---

## 🏠 Homework
- **Textbook “Test Yourself”** from pp. 141–154 (items on ambiguity and garden-paths). 
- **Short write-up (≈150–200 words)**: Compare **syntax-first** vs **constraint-based** using **one** sentence from class; predict how **context** or **prosody** would change the difficulty.

---

## 🧩 Self-Check Questions

**Q1.** What is a **garden-path sentence** and why does it occur? 
&lt;!-- A garden-paths lures the parser into an initially plausible but wrong structure during incremental parsing; later input forces reanalysis, causing slowdown. -->
&lt;!--
**Q2.** Define **Minimal Attachment** and **Late Closure** in one sentence each. -->
&lt;!-- Minimal Attachment: prefer the parse with the fewest new nodes (simplest structure). Late Closure: attach new material to the phrase currently being processed whenever grammar allows. -->
&lt;!--
**Q3.** Give one example where **prosody** reduces a garden-path. -->
&lt;!-- Adding a pause/comma: “While the man hunted, the deer ran into the woods.” The boundary discourages attaching "the deer" as the object of "hunted." -->
&lt;!--
**Q4.** How would a **syntax-first** account differ from a **constraint-based** account for “The horse raced past the barn fell”? -->
&lt;!-- Syntax-first: parser initially builds a main-clause parse (MA/LC), then reanalyses to a reduced relative when "fell" arrives. Constraint-based: with enough cues (frequency, plausibility, prosody), the reduced-relative parse can be activated earlier, reducing garden-pathing. -->
&lt;!--
**Q5.** Why do **verb biases** matter for ambiguity? -->
&lt;!-- Verbs differ in preferred complements; if a verb strongly prefers a direct object or a clause, the parser's expectations shift, affecting early attachment choices and garden-path likelihood. -->
&lt;hr>
&lt;!--
## 🧰 Key Terms
**Incremental parsing**, **Temporary ambiguity**, **Garden-path**, **Reanalysis**, **Minimal Attachment**, **Late Closure**, **Reduced relative**, **Verb bias**, **Syntax-first**, **Constraint-based/interactive**, **Prosody**.

---

## 🌐 Optional Resources
- Short explainers/demos of **garden-path sentences** and parsing heuristics. 
- Audio examples showing **prosodic disambiguation** (pause/comma intonation). 
- Beginner blog posts on **why “The horse raced past the barn fell” is hard**.

---

### ✅ How to use these notes
- **Before class:** preview examples; try reading them aloud with/without pauses. 
- **During class:** name the **heuristic** behind your first analysis. 
- **After class:** practice rewriting garden-paths with **disambiguating cues** (that, commas, different verbs).


-->
&lt;!--
## 📘 Overview

This week focuses on **discourse-level comprehension**: how we go from understanding isolated sentences to constructing **integrated mental models** of entire narratives or conversations. You'll learn three influential models—**Construction-Integration**, **Structure Building**, and **Event Indexing**—and examine how knowledge, memory, and coherence contribute to understanding stories and discourse.

---

## 🧠 Core Concepts

### What Is Discourse Processing?

- Involves understanding **connected text** or conversation, not just isolated sentences.
- Requires building a **situation model**: a mental simulation of the events described.

---

### Model 1: Construction–Integration Theory

| Stage | Description |
|-------|-------------|
| **Construction** | Generate all possible meanings (including irrelevant or contradictory ones). |
| **Integration** | Use context and prior knowledge to suppress irrelevant meanings and **integrate** consistent ideas. |

- Influenced by **spreading activation** and **network models**.
- Highlights importance of **text coherence** and **prior knowledge** in reducing ambiguity.

---

### Model 2: Structure Building Framework

| Component | Role |
|----------|------|
| **Laying a Foundation** | Readers begin building a representation when they encounter a new topic or idea. |
| **Mapping** | New info is integrated if related to the current structure. |
| **Shifting** | If unrelated, a **new substructure** is created (topic shift, tense change, etc.).

- **Enhancement** strengthens relevant nodes; **suppression** reduces activation of unrelated ideas:contentReference[oaicite:0]{index=0}.

---

### Model 3: Event Indexing Model

- Readers track **5 key dimensions** to understand a situation:

| Dimension | Example |
|-----------|---------|
| **Time** | Do events occur at the same time? |
| **Space** | Are characters in the same location? |
| **Entity** | Are the same people or objects involved? |
| **Causality** | Are events causally linked? |
| **Intentionality** | Are goals or motivations shared? |

- **Similarity across dimensions** helps maintain coherence and facilitates comprehension:contentReference[oaicite:1]{index=1}.

---

### Causal and Coherence Inference

- We use **background knowledge** and **semantic cues** to make inferences:
 - Temporal ordering
 - Cause-effect relationships
 - Goal fulfillment
- **Bridging inferences**: connect sentences across gaps.
- **Elaborative inferences**: enrich the model with relevant but unstated info.

---

## 📚 Reading

- Traxler (2012), Chapter 5: *Discourse Processing* (pp. 187–210)

---

## 🏷️ Key Terms

| Term | Definition |
|------|------------|
| **Discourse** | Connected sequences of language (text or talk) |
| **Situation Model** | Mental representation of what a discourse describes |
| **Construction-Integration Model** | Framework describing how meanings are initially constructed and then refined |
| **Structure Building** | A model of comprehension involving foundation, mapping, and shifting |
| **Event Indexing** | Model focusing on dimensions tracked during discourse understanding |

---

## 🧪 Examples &amp; In-Class Activities

### 🔁 Coherence Building Challenge

- Present scrambled story events (e.g., “She cried. He left. They fought.”).
- Students reorder and justify the most **coherent** narrative.

### 📚 Story Retelling

- Read a short paragraph aloud.
- In pairs, students retell the story to a peer from memory.
- Discuss what parts were emphasized, omitted, or inferred.

### 🧩 Event Indexing Game

- Provide mini-stories that vary one event dimension (e.g., space or causality).
- Ask students which version would be harder to follow and why.

---

## ❓ Self-Check Questions

1. What are the three major models of discourse processing introduced in this chapter?
2. How do readers use world knowledge to fill in gaps in discourse?
3. What is a situation model, and how is it constructed?
4. What dimensions are tracked according to the Event Indexing Model?
5. How does coherence affect memory and comprehension?

---

## 🧠 Practice Prompt (Adapted)

> Read the following two-sentence discourse: 
> “Jason dropped the vase. The floor was wet.” 
> - What kind of inference do you need to make to understand this connection? 
> - Which model(s) of discourse processing best explain this?

---

## 🔁 Related Chapters

- Chapter 4: *Sentence Processing* (input to discourse)
- Chapter 6: *Reference* (linking referents across discourse)
--></description></item><item><title>Week 6: 🧠 Word Meaning &amp; the Brain</title><link>https://zjpsycholin.github.io/psycholinguistics/week06/</link><pubDate>Mon, 19 May 2025 10:03:44 -0400</pubDate><guid>https://zjpsycholin.github.io/psycholinguistics/week06/</guid><description>&lt;p>To be published &amp;hellip;&lt;/p>
&lt;!--
## 📘 Overview
When you read or hear a word like **bank**, your brain must rapidly choose the right meaning (money? river?). This week explores **how word meaning is represented and selected**, how **context** and **frequency/bias** guide **ambiguity resolution**, how meanings are linked in **semantic networks**, and what **brain measures** (like the **N400**) tell us about semantic processing. We’ll analyze ambiguous sentences, build small semantic networks, and discuss how L1/L2 experience shapes meaning access.

---

## 🎯 Learning Goals
By the end of Week 6, you should be able to:

- Distinguish **homonymy** vs **polysemy** and explain why it matters for processing.
- Describe how **frequency/bias** and **context** influence **lexical ambiguity resolution**.
- Explain core ideas of **semantic network** organization (feature overlap, associative links).
- Interpret classic **priming** and **N400** findings as evidence for semantic activation.
- Apply these ideas to analyze ambiguous sentences in English and compare with Chinese.

---

## 📖 Required Reading
- **Traxler (1st ed.), Chapter 3, pp. 113–129** — *Word Processing* (meaning and semantic access).

---

## 🔑 Key Concepts &amp; Mini-Explanations

### 🧭 Homonymy vs. Polysemy
- **Homonymy**: *bank* (money) vs *bank* (river) — **unrelated** meanings share form. 
- **Polysemy**: *paper* (material / newspaper / article) — **related** senses share a core concept. 
- **Processing note**: Polysemous words often show **processing advantages** (shared core features), while homonyms rely more on **context** to select the right entry.

### 🎯 Ambiguity Resolution
- **Meaning dominance (bias)**: Common senses (e.g., *bank–money*) are selected more quickly than rare senses (*bank–river*) unless **strong context** flips the bias. 
- **Context timing**: **Early** supportive context can pre-activate the appropriate meaning; **late** context forces re-interpretation (slower, more errors). 
- **Subordinate-bias effect**: Rare meanings need **clear cues**; without them, readers/listeners default to dominant senses.

### 🌐 Semantic Networks &amp; Features
- Words are linked by **category** (CAT–DOG), **association** (BREAD–BUTTER), and **shared features** (BIRD–BAT: wings/flies). 
- **Spreading activation**: Seeing *doctor* partially activates *nurse*, speeding recognition (**semantic priming**). 
- **Feature verification**: True-feature decisions (e.g., “A robin has wings”) are faster than less typical ones (“A penguin has wings”).

### 🧠 Brain Signatures of Meaning
- **N400** (ERP component): larger (more negative) around ~400 ms for **semantic mismatches** (e.g., “He spread the warm bread with **socks**”). 
- **Context reduces N400**: predictable words elicit **smaller** N400s; unrelated or anomalous words elicit **larger** ones. 
- **Regions** (big picture): temporal-parietal areas (e.g., **posterior temporal**, **angular gyrus**) often implicated in semantic integration; **anterior temporal** supports conceptual combination.

### 🧩 L1–L2 &amp; Cross-Linguistic Notes
- **Chinese homophones** (音同形不同) can increase ambiguity; characters disambiguate in writing. 
- **English polysemy** (e.g., metaphorical extensions) may require more context for L2 learners. 
- Strategy: attend to **local cues** (adjacent words) and **global topic** to bias the right sense.

---

## 📝 Pre-Class Activities
1. **Read** pp. 113–129 and list **three ambiguous words** you encountered (mark **dominant** vs **subordinate** meanings). 
2. **Mini network**: Choose one target (e.g., *head*) and draw **8 connected nodes** (category, parts, uses, metaphors). 
3. **Quick sentences**: Write two short contexts for *bank* that **unambiguously** force each meaning (money vs river).

---

## 💬 In-Class Activities

### 1) Ambiguity Lab: Dominant vs Subordinate (15 min)
- **Pairs**: Classify 12 sentences by intended sense (homonymy set: *bank, bark, bat, match*). 
- **Task**: Decide if context is **strong/weak**, and whether a dominant or subordinate sense is selected. 
- **Predict**: Which will take longer to understand? (Subordinate with weak context).

**Sample set** 
1. *The hikers sat on the bank and watched the fish.* 
2. *I deposited my paycheck at the bank after class.* 
3. *The guard saw a bat fly out of the cave.* 
4. *The batter held the bat tightly and swung.* 
5. *The dog’s bark echoed across the yard.* 
6. *The pine’s bark protects it from insects.* 
7. *We lit the match and the candle flickered.* 
8. *It was a perfect match between the two finalists.*

### 2) Context Strength Challenge (12 min)
- **Groups of 3–4**: Rewrite **two** weak-context items to **strongly bias** the subordinate sense (e.g., *bank–river*). 
- **Share** and **vote**: Which context makes the subordinate sense most obvious?

### 3) Semantic Network Sprint (12 min)
- **Teams** build a network for **one target** (e.g., *head*, *light*, *spring*). 
- Include: **category**, **parts**, **actions**, **associations**, **metaphors**. 
- Add **three edges** that explain likely **priming** routes (why would A speed B?).

### 4) N400 Thought Experiment (8 min)
- **Pairs**: For each sentence stem, choose a **predictable**, **related-but-odd**, and **anomalous** completion. 
- **Predict** N400 size (small ↔ large) and justify.

**Stems** 
- *She stirred her coffee with a…* 
- *After the marathon, he drank a bottle of…* 
- *They decorated the cake with…*

### 5) Whole-Class Discussion (8 min)
- Why is **polysemy** often easier than **homonymy**? 
- When does **context** arrive too late to prevent confusion? 
- L2 angle: Which cues do you rely on first in English?

---

## 🔁 Post-Class Review
- **One-pager**: Explain the **subordinate-bias effect** with one example from class and one you found yourself. 
- **Network tidy-up**: Redraw your semantic network more clearly (show **strong vs weak** links). 
- **Reflection (100–120 words)**: Describe a moment when context **rescued** you from a wrong meaning.

---

## 🏠 Homework
- **Textbook “Test Yourself”** (Ch. 3, pp. 113–129): items on ambiguity and semantic access. 
- **Short write-up (≈150–200 words)**: Compare **homonymy vs polysemy** using two words from class; predict differences in processing and N400 patterns. 
- **Optional mini-priming**: Create 6 prime–target pairs (2 semantic, 2 associative, 2 unrelated). Ask a friend to read targets after primes; collect subjective “fast/slow” ratings and summarize.

---

## 🧩 Self-Check Questions

**Q1.** What’s the difference between **homonymy** and **polysemy** for processing? -->
&lt;!--*A:* Homonyms have **unrelated** meanings that compete until context resolves them; polysemous senses share features and often show **processing advantages**.*-->
&lt;!--
**Q2.** What is the **subordinate-bias effect**? -->
&lt;!--*A:* Readers default to the **dominant** meaning unless strong context cues support the **subordinate** meaning, which otherwise is slower/harder to select.*-->
&lt;!--
**Q3.** How do **semantic networks** explain priming? -->
&lt;!--*A:* Activation spreads along **meaningful links**, so related targets are recognized **faster** than unrelated ones.*-->
&lt;!--
**Q4.** What does the **N400** index? -->
&lt;!--*A:* The ease/difficulty of **semantic integration**; **larger** for unexpected or anomalous words, **smaller** for predictable ones.*-->
&lt;!--
**Q5.** Why might Chinese–English bilinguals experience more ambiguity for **homophones**? --> 
&lt;!--*A:* High homophone density in Chinese increases potential mappings; English requires **context** and **topic** to constrain selection quickly.*-->
&lt;hr>
&lt;!--
## 🧰 Key Terms
**Homonymy**, **Polysemy**, **Dominant/Subordinate meaning**, **Context strength**, **Semantic priming**, **Associative links**, **Feature overlap**, **Spreading activation**, **N400**, **Semantic integration**, **Anterior temporal lobe**, **Angular gyrus**.

---

## 🌐 Optional Resources
- Short explainers on **N400** and semantic priming (intro videos/articles). 
- Interactive tools for building **semantic networks** (mind-mapping apps). 
- Corpora or frequency lists to check **dominant meanings** in context.

---

### ✅ How to use these notes
- **Before class:** read and prepare your ambiguous word list + contexts. 
- **During class:** push for **strong contextual cues** and justify your choices. 
- **After class:** refine your network and relate it to **priming** and **N400** predictions.

-->
&lt;!--
## 🧩 Overview

This week we begin exploring **sentence processing**—how we comprehend sentences in real time. You'll learn about **parsing**, **garden-path sentences**, and the strategies readers and listeners use to interpret ambiguous structures. We'll also examine how researchers study these processes using reaction times and eye-tracking.

-->
&lt;!-- 
## 📘 Core Topics

- The challenge of **incremental parsing**
- Temporary ambiguity and **garden-path effects**
- Two-stage parsing models:
 - **Syntax-first approach**
 - **Constraint-based approach**
- Empirical methods in sentence processing

---

## ❓ Guiding Questions

- What strategies do we use to parse sentences as we read or listen?
- Why do some sentences lead us down the wrong path?
- How does the brain decide between multiple syntactic interpretations?
- What evidence supports the idea that meaning and context influence parsing?

---

## 🔍 Key Concepts

### Garden-Path Sentences
- Sentences that start off seeming to mean one thing but end up meaning another.
- Example: 
 *“The horse raced past the barn fell.”* 
 Initially misparsed due to temporary ambiguity.

### Parsing Models

#### Syntax-First (Two-Stage) Model
- Proposed by **Frazier &amp; Rayner (1982)**.
- Initial parsing is based solely on syntactic structure.
- Semantics and context are used **only afterward** to revise incorrect parses.

#### Constraint-Based Models
- Parsing is influenced from the start by **multiple sources** of information: syntax, semantics, frequency, and context.
- Considers **probabilistic cues** and parallel representations.

### Empirical Evidence
- Eye-tracking and self-paced reading show increased processing time at disambiguation points in garden-path sentences.
- The more unexpected a structure is, the more reanalysis is needed.

---

## 🧠 Before Class

### 1. Pre-Reading Prompt 
Read these two sentences:
- *“The old man the boats.”*
- *“While the man hunted the deer ran into the woods.”*

Try to explain why they are difficult to understand at first.

### 2. Think About
- What strategies do you use when a sentence doesn't make sense immediately?
- Can you think of a time when you misunderstood someone due to how they phrased something?

---

## 🏫 In-Class Activities

### 🧪 Garden-Path Sentence Game
- Identify and fix misparsed garden-path sentences in groups.
- Use provided sentence strips with ambiguities to test each other.

### 🔍 Parsing Strategy Debate
- Split into two teams:
 - One defends the **syntax-first** view.
 - One supports the **constraint-based** approach.
- Use examples from the textbook to support your case.

### 🧠 Parsing Demo
- Read aloud ambiguous sentences.
- Students press a key or raise hands when they feel confusion or reanalysis is needed.
- Discuss what made certain parts harder to process.

---

## 📝 After Class

### 1. Review Questions
- What is the difference between a syntax-first and a constraint-based model?
- Why do garden-path sentences confuse us?
- What kinds of information (besides syntax) help us parse sentences in real time?

### 2. Practice Task
Write two of your own garden-path sentences. Try to trick a friend! Then explain where the ambiguity lies and how it resolves.

---

## 🎯 Additional Resources

- 📺 [Garden-Path Sentences Explained – AsapSCIENCE](https://www.youtube.com/watch?v=8QyVZrV3d3g) *(3-min video)*
- 🎧 *Lingthusiasm Podcast*, Ep. 30: “Why do we garden-path ourselves?”
- 📖 Supplement (optional): 
 Frazier, L. (1987). “Syntactic processing: Evidence from Dutch.” *Natural Language and Linguistic Theory*.

---

## 🧠 Summary

> Sentence processing is rapid, incremental, and sometimes error-prone. The brain uses different strategies—some focused on syntactic structure, others more holistic—to interpret sentence meaning as it unfolds. Garden-path effects reveal the dynamic nature of parsing and the influence of context.
-->
&lt;!--
## 📘 Overview

When we read or hear a sentence, our brain rapidly organizes words into grammatical structure—a process known as **parsing**. This week introduces the classic models of how parsing works, explores **garden-path sentences**, and compares **modular (two-stage)** and **interactive (constraint-based)** approaches. We also discuss how frequency, semantics, prosody, and visual context guide real-time sentence comprehension.

---

## 🧠 Core Concepts

### What Is Parsing?

- Parsing is the **unconscious process** of assigning **syntactic structure** to incoming linguistic input.
- The parser must resolve **temporary ambiguities** and do so **incrementally**—word by word.

---

### Garden-Path Sentences

- Ambiguities lead to **misanalysis** during parsing:
 > “The horse raced past the barn fell.” 
- Requires **reanalysis**, which increases processing time.

---

### Two-Stage Models (Modular Parsing)

| Feature | Description |
|---------|-------------|
| **Syntax-first** | Parsing is guided by grammatical structure alone, before semantics or context intervene. |
| **Minimal Attachment** | Choose the parse with **fewest nodes** (simplest structure). |
| **Late Closure** | Attach new words to the **current phrase** if possible. |
| **Serial** | Only one parse is pursued at a time. Reanalysis happens when it fails. |

- Empirical support: eye-tracking shows longer fixations when initial parse fails.

---

### Constraint-Based Models (Interactive Parsing)

| Feature | Description |
|---------|-------------|
| **Multiple constraints** | Parsing is influenced by **syntax, semantics, frequency, discourse**, etc. |
| **Parallel processing** | Multiple interpretations are considered simultaneously. |
| **Probabilistic** | Readers/listeners evaluate likelihoods based on prior experience. |

- Supported by studies showing that **frequency** of verb argument structure, **story context**, and **visual scenes** can affect initial parse:contentReference[oaicite:0]{index=0}.

---

### Key Influences on Parsing

| Factor | Effect |
|--------|--------|
| **Story Context** | Helps disambiguate otherwise ambiguous sentences |
| **Verb Subcategorization Frequency** | Frequent syntactic frames are preferred |
| **Prosody** | Intonation affects syntactic grouping in spoken language |
| **Semantic Plausibility** | Meaning constraints bias interpretation |
| **Visual Context** | Real-world scenes affect syntactic interpretation (e.g., Tanenhaus et al.)

---

## 📚 Reading

- Traxler (2012), Chapter 4: *Sentence Processing* (pp. 141–166)

---

## 🏷️ Key Terms

| Term | Definition |
|------|------------|
| **Parsing** | The process of assigning syntactic structure during comprehension |
| **Garden-path Sentence** | A sentence that leads the reader to an incorrect parse |
| **Minimal Attachment** | Preference for syntactic structures with fewer nodes |
| **Late Closure** | Tendency to attach new information to the current clause |
| **Constraint-Based Parsing** | Parsing model that uses multiple sources of information simultaneously |

---

## 🧪 Examples &amp; In-Class Activities

### 🧩 Garden-Path Sentence Game

- Present sentences like:
 > “The old man the boats.”
 > “The man whistling tunes pianos.”
- Ask students to find the ambiguity and reparse them.

### 📈 Minimal Attachment Exercise

- Students diagram sentence structures with different parses.
- Predict which one the parser would favor using two-stage rules.

### 🎬 Constraint-Based Video Clip

- Show short clips where context affects sentence interpretation.
- Relate to visual world paradigm (e.g., eye movements during parsing).

### 🔁 Reanalysis Reaction Task

- Read ambiguous sentences and note when rereading is necessary.
- Connect delays to eye-tracking evidence in parsing studies.

---

## ❓ Self-Check Questions

1. What is the difference between two-stage and constraint-based parsing models?
2. How do minimal attachment and late closure guide initial parsing?
3. What types of evidence support interactive, constraint-based parsing?
4. What makes garden-path sentences difficult to understand?
5. How do frequency and context influence syntactic ambiguity resolution?

---

## 🧩 Practice Prompt (Adapted)

> Choose one of the following sentences and explain why it is hard to parse: 
> “The cotton clothing is made of grows in Mississippi.” 
> What parsing strategy might the brain use first? 
> How would a constraint-based model help resolve it?

--></description></item><item><title>Week 5: 📖 Lexical Access</title><link>https://zjpsycholin.github.io/psycholinguistics/week05/</link><pubDate>Thu, 15 May 2025 10:03:44 -0400</pubDate><guid>https://zjpsycholin.github.io/psycholinguistics/week05/</guid><description>&lt;p>To be published &amp;hellip;&lt;/p>
&lt;!--
## 📘 Overview
When you see or hear a word, recognition feels instant—but under the hood your brain is racing through **candidates**, using **frequency**, **spelling/sound similarity**, **morphology**, and **meaning** to settle on the best match. This week introduces the **mental lexicon** and shows how psychologists probe it with tasks like **lexical decision** and **priming**. You’ll run quick in-class demos to feel **frequency**, **neighborhood**, and **semantic/morphological priming** effects yourself, then connect your observations to classic **models of word recognition**.

---

## 🎯 Learning Goals
By the end of Week 5, you should be able to:

- Describe the **mental lexicon** and distinguish **lemma** (meaning/grammar) from **lexeme** (form/sound).
- Explain key effects in word recognition: **frequency**, **age of acquisition**, **neighborhood density**, and **morphological structure**.
- Define and exemplify **semantic**, **associative**, **form**, and **masked** **priming**.
- Run and interpret a simple **lexical decision** and **priming** demo (predict RT patterns; explain why).
- Compare core ideas of **recognition models** (Logogen/activation threshold, Cohort, Interactive Activation/IA; brief link to TRACE for speech).

---

## 📖 Required Reading
- **Traxler (1st ed.), Chapter 3, pp. 79–113** — *Word Processing*.

---

## 🔑 Key Concepts &amp; Mini-Explanations

### 🧠 Mental Lexicon: Structure &amp; Access
- **Lemma vs. Lexeme**: a word’s **lemma** encodes semantics + syntactic category; its **lexeme** encodes **orthography/phonology** (letters/sounds, syllables, stress).
- **Organization** is **distributed**: words connect by **meaning** (cat–dog), **form** (cat–cap), and **morphology** (teach–teacher–teaches).

### ⏱️ Lexical Decision (Word vs. Nonword)
- Task: “Word?” → **YES/NO**. 
- **Word frequency**: high-frequency words (e.g., *people*) recognized **faster** than rare ones (e.g., *pergola*). 
- **Age of acquisition**: words learned earlier are generally faster. 
- **Pseudowords vs. illegal strings**: *blat* (looks word-like) slows decisions more than *xqmt*.

### 🧩 Neighborhood &amp; Similarity
- **Orthographic/phonological neighborhood**: number of one-letter/one-phoneme “neighbors” (cat→cap/cot/can). 
- **Dense neighborhoods** can **hinder** lexical decision (competition) or sometimes **help** recognition in other tasks (more activation to settle). 
- **Big idea**: access involves **activation + competition** among similar candidates.

### 🌐 Priming: Speed from Relations
- **Semantic priming**: related primes (DOG→CAT) **speed** target recognition vs. unrelated (PEN→CAT). 
- **Associative priming**: learned associations (BREAD→BUTTER). 
- **Form/phonological priming**: overlap in letters/sounds (CORN→CORD) yields small, task-dependent effects. 
- **Morphological priming**: TEACH→TEACHER (or TEACHER→TEACH) facilitates beyond form overlap → evidence for **decomposition**. 
- **Masked priming**: very brief, masked primes can still speed related targets → **early, automatic** activation.

### 🧠 Models of Word Recognition (Spotlight)
- **Logogen / Threshold models**: each word has a **resting activation** (higher for frequent words); input raises activation until a **threshold** is reached. 
- **Cohort (spoken words)**: initial sounds activate a **cohort** of candidates (spa- → *spin, spot, space*), then **narrow** as input unfolds. 
- **Interactive Activation (IA, visual words)**: **features → letters → words** interact **bidirectionally**; explains word/letter effects and **top-down** influence. 
- **TRACE (spoken)**: an IA-style model for speech (features ↔ phonemes ↔ words) with **competition** and **context**.

---

## 📝 Pre-Class Activities
1. **Read** pp. 79–113 and note a real example of **semantic priming** you’ve felt in daily life. 
2. **Frequency check**: Pick 5 English words and guess their frequency (high/low); predict which are faster in lexical decision. 
3. **Mini network**: Draw a small **semantic network** for *school* (at least 8 nodes; label strongest links).

---

## 💬 In-Class Activities

### 1) Lexical Decision Task Demo (12 min)
- Instructor flashes a list (mixed **high/low frequency**, **pseudowords**). 
- Students tap **YES**/**NO** on their phones (self-timed); write down 3 **slow** items. 
- **Debrief**: Were low-frequency and pseudowords slower? Any confusing pseudowords (*frind*)?

**Sample set (print or project):** 
`people, garden, nephew, pergola, axiom, fjord; frind, glabe, slint, xqmt`

### 2) Semantic Priming Mini-Experiment (15 min)
- Pairs: Partner A shows **prime→target** quickly (paper slips or slides). Partner B says target aloud and estimates speed (fast/slow). 
- **Sets**: Related (DOG→CAT; DOCTOR→NURSE), Unrelated (PEN→CAT), Morphological (TEACH→TEACHER), Form (CORN→CORD). 
- **Prediction**: Related &amp; morphological **faster** than unrelated; form effects smaller/variable.

### 3) Neighborhood Challenge (10 min)
- For targets (*cat, bed, read, cake*), each group lists as many **one-letter neighbors** as possible in 2 minutes, then **predicts** which targets will be **slowest** in lexical decision (more neighbors ⇒ more competition). 
- Quick test with a short second list if time.

### 4) Morphology Puzzle (10 min)
- Identify **stems** and **affixes** in items like *teacher, happiness, unreadable, miscalculate*. 
- Decide which prime–target pairs should show **morphological priming** beyond form (e.g., HAPPY→HAPPINESS vs. CORN→CORNER).

### 5) Models Quick-Stations (10 min)
- Small groups rotate through **three posters**: Logogen, Cohort/TRACE (spoken), Interactive Activation (visual). 
- For each model, write **one strength**, **one limitation**, and **one prediction** about: frequency, neighborhood, or priming.

---

## 🔁 Post-Class Review
- **Summarize your results** from Activities 1–2 in 5–7 bullet points (which conditions were fastest/slowest and why). 
- **Reflection (100–120 words):** Which model best fits your observations today? Give one concrete example. 
- **Optional**: Re-run a tiny priming test with friends/family (3 related vs 3 unrelated pairs) and compare impressions.

---

## 🏠 Homework
- **Textbook “Test Yourself”** (Ch. 3, pp. 79–113): items on lexical decision, priming, morphology. 
- **Short write-up (≈150–200 words):** Explain a **semantic vs. morphological** priming contrast you observed (or expected). What does it imply about **decomposition**? 
- **Extension (optional):** Build a 10-node **semantic network** for a new word (e.g., *travel*); label link strengths.

---

## 🧩 Self-Check Questions

**Q1.** Why are **high-frequency** words recognized faster? -->
&lt;!--*A:* They have **higher resting activation** and/or lower thresholds, so they reach recognition faster in activation/competition frameworks.*-->
&lt;!--
**Q2.** What’s the difference between **semantic** and **associative** priming? -->
&lt;!--*A:* Semantic = conceptual/category relatedness (CAT–DOG); associative = learned co-occurrence (BREAD–BUTTER). Many pairs are both.*-->
&lt;!--
**Q3.** What pattern suggests **morphological decomposition**? -->
&lt;!--*A:* Transparent pairs (TEACH–TEACHER) prime more than pure form overlap (CORN–CORD), even when orthography is similar.*-->
&lt;!--
**Q4.** How does **neighborhood density** affect lexical decision? -->
&lt;!--*A:* Dense neighborhoods can **slow** decisions due to competition among similar candidates.*-->
&lt;!--
**Q5.** What’s **masked priming**, and why is it important? -->
&lt;!--*A:* A very brief, masked prime speeds recognition without conscious awareness—evidence for **early, automatic** activation in access.*-->
&lt;hr>
&lt;!--
## 🧰 Key Terms
**Mental lexicon**, **Lemma/Lexeme**, **Lexical decision**, **Word frequency**, **Age of acquisition**, **Neighborhood density**, **Semantic priming**, **Associative priming**, **Form/orthographic priming**, **Morphological priming**, **Masked priming**, **Activation threshold**, **Competition**, **Logogen**, **Cohort**, **Interactive Activation (IA)**, **TRACE**.

---

## 🌐 Optional Resources
- Short explainers/demos of **lexical decision** and **priming** (searchable videos). 
- Interactive tools for **word frequency** and **neighborhood** lookup (e.g., lexical databases). 
- Blog posts or beginner articles on **morphological decomposition** and **N400** (semantic processing).

---

### ✅ How to use these notes
- **Before class:** read, make predictions for the demos (frequency/priming). 
- **During class:** time yourself honestly; note which items felt “sticky.” 
- **After class:** connect your observations to model predictions; revise your semantic network.

-->
&lt;!--
## 📘 Overview

How does your brain know that the word “bank” can mean both a place for money and the side of a river? This week we’ll explore **how words are linked to meanings**, how the mind resolves ambiguous words, and what this reveals about the organization of the mental lexicon. We’ll also learn how semantic networks connect concepts and how brain evidence supports these models.

---

## 🎯 Learning Goals

By the end of this week, you should be able to:

- Explain how **word meanings** are stored and accessed in the brain.
- Describe how the brain resolves **ambiguity** in word meaning.
- Understand and illustrate **semantic networks** and spreading activation.
- Discuss experimental evidence for semantic priming and network structure.
- Connect behavioral data and brain studies to models of meaning.

---

## 📖 Required Reading

- **Chapter 3 (pp. 113–129)** from *Introduction to Psycholinguistics: Understanding Language Science* by Matthew Traxler.

---

## 🧠 Core Concepts

### 🗝️ Lexical Ambiguity

- **Lexical ambiguity**: Many words have multiple meanings (“bat”—an animal or a piece of sports equipment).
- **How is ambiguity resolved?**
 - **Context** is crucial. The brain uses sentence or situation cues to select the intended meaning.
 - **Time course**: Both meanings may be briefly activated before context narrows down the choice.

---

### 🔗 Semantic Networks

- **Semantic networks**: Concepts are stored as nodes connected by links representing relationships (e.g., *dog* is linked to *cat*, *pet*, *bark*).
- **Spreading activation**: Activating one concept (e.g., “nurse”) automatically spreads to related nodes (e.g., “doctor”).
- **Support**: Evidence from **semantic priming** and reaction time experiments shows that related words are accessed faster.

---

### 🧪 Experimental Findings

- **Semantic priming**: Recognizing a word (e.g., “bread”) is faster when it follows a related word (e.g., “butter”).
- **Ambiguity resolution studies**:
 - Eye-tracking and reaction times reveal both meanings of ambiguous words are initially activated.
 - Brain imaging (fMRI, ERP) shows different neural patterns for ambiguity and meaning selection.

---

### 🧬 Word Meaning in the Brain

- **Brain regions**: Areas in the temporal and frontal lobes are critical for processing meaning.
- **Neuropsychology**: Some brain injuries cause **semantic deficits** (e.g., difficulty naming or understanding related concepts).

---

## 📝 Pre-Class Activities

1. 📖 **Read Chapter 3 (pp. 113–129)**, focusing on ambiguity, semantic networks, and priming studies.
2. 💡 **Quick reflection**: 
 > Write down three words that have more than one meaning. For each, try to give sentences showing the different meanings.
3. 🎧 **Optional video**: 
 [How the Brain Resolves Ambiguity (YouTube)](https://www.youtube.com/watch?v=r2ZlKgqY9wE)

---

## 💬 In-Class Activities

- 🧠 **Ambiguous Sentence Challenge**: Interpret sentences like "He saw her duck."
- 🔄 **Semantic Priming Demo**: Predict and observe speed differences for related/unrelated word pairs.
- 🔗 **Semantic Network Mapping**: Create a network diagram for a word (e.g., “bank”) and its related concepts.
- 🧩 **Case Study Discussion**: Analyze neuropsychological patient data (e.g., semantic dementia).
- 👥 **Mini-group Task**: Solve short ambiguity puzzles and present your reasoning.

---

## 🔁 Post-Class Review

1. ✍️ **Short answer**: 
 > Describe one experiment that demonstrates spreading activation in a semantic network.
2. 🧠 **Reflection**: 
 > How do you resolve ambiguous words in your first or second language? Does it feel automatic or effortful?
3. 📄 **Optional extension**: 
 Try [this semantic network tool](https://www.visuwords.com/) to explore connections for a word of your choice.

---

## 🏠 Homework

- 📖 Re-read the section on **semantic priming** and **ambiguity**.
- 📝 Complete at least two *Test Yourself* questions from this section.
- 🧠 **Mini Assignment**: 
 > Choose an ambiguous word in English or Chinese. Write two short stories, each using a different meaning. Highlight how context clarifies which meaning is intended.

---

## 🔜 Looking Ahead

Next week, we’ll look at how your mind builds meaning **beyond individual words**—exploring how sentences are parsed and interpreted in real time!

---





&lt;!--
## 📘 Overview

In this session, we move beyond lexical access and focus on how the brain **represents**, **interprets**, and **resolves** word meanings. You’ll learn how context and experience shape the comprehension of ambiguous or polysemous words, and how neuroscientific methods help uncover the distributed nature of meaning representation in the brain.

---

## 🧠 Core Concepts

### Lexical Ambiguity

- Words can have:
 - **Homonyms**: two unrelated meanings (e.g., *bank*)
 - **Polysemes**: related meanings (e.g., *paper* as material or article)
- Ambiguity must be resolved quickly and often unconsciously during comprehension:contentReference[oaicite:0]{index=0}.

---

### Context Effects

- **Contextual cues** help the brain suppress irrelevant meanings.
- Example: “The fisherman sat on the river bank.” → *bank* = riverbank
- **Priming** experiments show faster responses when contextually appropriate meanings are activated.

---

### Semantic Priming

- When a word like “doctor” is processed faster after seeing “nurse” than “bread”.
- Priming reveals the **organization** of semantic memory: related meanings are **closely connected** in the mental lexicon:contentReference[oaicite:1]{index=1}.

---

### Representation of Word Meaning

| Theory | Key Idea |
|--------|----------|
| **Semantic Feature Theory** | Words are represented by sets of semantic features (e.g., +ANIMATE, +TOOL) |
| **Prototype Theory** | Meaning is organized around a prototypical example |
| **Distributed Semantic Representations** | Meaning is spread over networks of shared features |

---

### Embodied &amp; Distributed Views (Revisited)

- Meaning is **not localized** to one area; it is **distributed** across sensory, motor, and associative regions.
- E.g., reading “kick” activates **motor areas** in the leg-related cortex:contentReference[oaicite:2]{index=2}.
- Supported by fMRI studies and TMS (transcranial magnetic stimulation) experiments.

---

## 📚 Reading

- Traxler (2012), Chapter 3: *Word Processing* (pp. 113–129)

---

## 🏷️ Key Terms

| Term | Definition |
|------|------------|
| **Semantic Priming** | Faster processing of a word due to prior exposure to a semantically related word |
| **Lexical Ambiguity** | The condition of a word having multiple meanings |
| **Semantic Feature Theory** | Theory that meaning is composed of sets of defining features |
| **Distributed Representation** | Theory that meaning is encoded in patterns across brain networks |
| **Embodied Semantics** | Theory that meaning involves sensorimotor experiences in the brain |

---

## 🧪 Examples &amp; In-Class Activities

### 🔍 Ambiguity Sorting

- Provide a list of ambiguous words.
- Students identify whether they are **homonyms** or **polysemes**.
- Discuss how context can disambiguate them.

### 🧪 Semantic Priming Demo

- Priming task: “bread–butter” vs. “bread–doctor”.
- Predict and explain differences in response time.

### 🧠 Brain and Meaning Activity

- Show neuroimaging data or brain diagrams from semantic studies.
- Discuss how different areas support **different semantic domains** (e.g., tools vs. emotions).

### ✍️ Sentence Completion Task

- Give ambiguous contexts:
 > “The ball was in the bank.” 
- Students provide disambiguating endings:
 > “… where it made a splash.” vs. “… guarded by a security camera.”

---

## ❓ Self-Check Questions

1. How does context help resolve lexical ambiguity?
2. What is semantic priming, and how is it tested?
3. How do the theories of word meaning (e.g., features, prototypes, distributed models) differ?
4. What evidence supports the idea that word meaning is embodied or distributed?
5. How do homonyms and polysemes differ in how the brain processes them?

---

## 🧩 Practice Prompt (Adapted)

> You are designing an experiment to test whether meaning is grounded in sensorimotor systems. 
> - What method might you use (e.g., fMRI, priming, TMS)? 
> - What predictions would support embodied semantics?

---

## 🔁 Related Chapters

- Chapter 2: *Speech Production and Comprehension*
- Chapter 4: *Sentence Processing* (context and ambiguity in syntactic domains)


-->
&lt;!--
## 🧠 Chapter 5 Lecture Notes: Word Recognition and Lexical Access I

How do we recognize words so rapidly and accurately — even in noisy environments or unfamiliar voices? This chapter explores how listeners perceive speech and how the brain retrieves word meaning from the mental lexicon.

---

## 📘 Core Topics &amp; Concepts

### 1. The Challenge of Speech Perception

* **Speech signal**: rapid, continuous, and variable across speakers and contexts
* **Coarticulation**: speech sounds overlap, yet we still identify discrete words
* **Lack of invariance**: no single acoustic cue always corresponds to a specific phoneme

> 🧠 Despite variability, humans are highly efficient speech processors

> 🎧 **In class**: We’ll hear examples of ambiguous or coarticulated speech and identify what makes them hard to parse.

---

### 2. Segmentation and Word Boundaries

* No clear boundaries between spoken words in fluent speech
* **Segmentation cues**:

 * **Stress patterns** (e.g., English uses trochaic stress: “MOther,” “BAby”)
 * **Phonotactic probabilities**: likelihood of certain sound sequences (e.g., /ng/ never starts a word in English)
 * **Statistical learning**: tracking transitional probabilities between syllables (e.g., “pre-tty-ba-by”)

> 🔍 **Research Spotlight**: Infants as young as 8 months use statistical learning to identify word boundaries in a speech stream

> 🧪 **In class**: We’ll try a brief segmentation demo using an artificial language stream.

---

### 3. The Mental Lexicon

#### Structure of the Lexicon

* **Lexical entries**: include word meaning, phonological form, syntactic category, morphological information
* Organized by:

 * **Frequency**: high-frequency words are recognized faster
 * **Neighborhood density**: number of similar-sounding words
 * **Semantic networks**: related meanings are linked

#### Accessing the Lexicon

* **Lexical access**: retrieving a word’s information during comprehension
* Influenced by bottom-up (sound) and top-down (contextual) cues

> 🔁 **Example**: In “He buttered the…” → likely access “toast” before “window” based on context

> 🧠 **In class**: We’ll map how word frequency and neighborhood density affect recognition.

---

### 4. Models of Spoken Word Recognition

* **Cohort Model**:

 * Word recognition starts from initial sounds (cohort)
 * Candidates are narrowed as more information comes in
* **TRACE Model**:

 * Interactive activation model
 * Includes competition between word candidates and top-down feedback

> 🎯 Both models explain different aspects of real-time word recognition

> 📊 **In class**: We’ll compare how the same input would be processed in the Cohort vs. TRACE model

---

## 🔁 Summary Table

| Concept | Description | Example |
| -------------------- | ------------------------------------- | --------------------------------------- |
| Coarticulation | Overlapping speech sounds | "don’t you" → "doncha" |
| Statistical Learning | Using probabilities to segment speech | "pretty baby" → likely break after “ty” |
| Lexical Frequency | Common words accessed faster | “the,” “cat,” “go” |
| Cohort Model | Early activation of word candidates | “ba…” → “baby,” “bacon,” “badge” |
| TRACE Model | Dynamic, interactive processing | Competing and reinforcing candidates |

---

## 📝 Self-Review Questions

1. Why is speech perception difficult from an acoustic perspective?
2. What cues do listeners use to segment continuous speech?
3. What types of information are stored in a lexical entry?
4. How does lexical frequency affect word recognition?
5. Compare the key ideas in the Cohort and TRACE models.

---

## 📂 In-Class Resources and References

* 🎧 *Coarticulation Demos*: “ice cream” vs. “I scream”
* 🎲 *Artificial Language Task*: Statistical learning demo
* 📄 *Lexical Access Chart*: Comparing word frequency and neighborhood effects
* 📚 *Optional Reading*: McClelland &amp; Elman (1986), “The TRACE model of speech perception”

---

> 📖 Reading: Chapter 5, pp. 90–117 from *Introduction to Psycholinguistics* by Traxler
--></description></item><item><title>Week 4: 👂 Speech Perception &amp; the Brain</title><link>https://zjpsycholin.github.io/psycholinguistics/week04/</link><pubDate>Fri, 09 May 2025 10:03:44 -0400</pubDate><guid>https://zjpsycholin.github.io/psycholinguistics/week04/</guid><description>&lt;p>To be published &amp;hellip;&lt;/p>
&lt;!--

## 📘 Overview
Spoken language reaches your ears as a fast, continuous, and noisy acoustic stream—yet you understand effortlessly. This week explains **why speech perception is hard** (coarticulation, variability, no clear word boundaries) and how the mind/brain **solves** it. We’ll demo the **McGurk effect**, explore **categorical perception** of stops (ba–pa), compare **major theories** (Motor Theory, Auditory Theory, TRACE), and outline **brain systems** for speech (temporal cortex; dorsal/ventral streams).

---

## 🎯 Learning Goals
By the end of Week 4, you should be able to:

- Explain why speech perception is **non-trivial** (speed, variability, lack of invariance, segmentation).
- Describe and illustrate **coarticulation** and **categorical perception** with concrete examples.
- Compare **Motor Theory**, **Auditory Theory**, and the **TRACE model** and state one prediction each.
- Explain the **McGurk effect** and why visual information influences what we hear.
- Identify key **brain areas/streams** involved in speech perception and their roles.

---

## 📖 Required Reading
- **Traxler (1st ed.), Chapter 2, pp. 51–71** — *Speech Perception*.

---

## 🔑 Key Concepts &amp; Mini-Explanations

### 🧩 Why Speech Perception Is Hard
- **Speed &amp; continuity:** ~10–15 phonemes/second; no consistent pauses between words.
- **Acoustic variability:** Same phoneme differs across speakers, rates, emotions, and contexts.
- **Coarticulation:** Overlap of gestures so sounds influence each other (anticipatory &amp; carryover).
- **Lack of invariance:** No one-to-one mapping from acoustic pattern to phoneme.
- **Segmentation problem:** Brain must infer word boundaries from context, stress, and knowledge.

### 🔄 Coarticulation
- **Definition:** Neighboring sounds overlap; current articulation anticipates upcoming segments. 
- **Example:** /k/ in **key** (fronted) vs **coo** (backed) changes the burst formant transitions. 
- **Implication:** Perceivers use context (surrounding vowels/places of articulation) to interpret ambiguous cues.

### 🎚️ Categorical Perception
- **Definition:** A continuous acoustic continuum (e.g., Voice Onset Time, VOT) is heard in **discrete categories** (e.g., /b/ vs /p/). 
- **Classic finding:** Sharp identification boundary and better discrimination **across** category boundary than **within** a category. 
- **Why useful:** Supports robust understanding despite acoustic variability.

### 🎬 The McGurk Effect (Audiovisual Integration)
- **Phenomenon:** Seeing **/ga/** dubbed onto **/ba/** often leads to hearing **/da/**. 
- **Takeaway:** Visual speech (lip movements) powerfully shapes auditory perception; effect grows in noise.

### 🧠 Theories of Speech Perception
- **Motor Theory:** We perceive **intended articulatory gestures**. Explains audiovisual influences (e.g., McGurk) and links to motor activation, but struggles with non-speech/perception in infants &amp; animals. 
- **Auditory (General) Theory:** Speech perception relies on **general auditory mechanisms** (not speech-specific). Supported by categorical-like effects in infants/animals; must account for context and visual influences. 
- **TRACE Model:** Interactive **connectionist** model with **features–phonemes–words**; supports **top-down** effects (e.g., Ganong effect: bias toward real words) and **bottom-up** accumulation. Predicts context can shift phoneme perception.

### 🧬 The Brain: Areas &amp; Streams
- **Superior temporal gyrus (STG)/superior temporal sulcus (STS):** early speech-sensitive processing and audiovisual integration. 
- **Wernicke’s area (posterior superior temporal):** speech comprehension; damage → fluent aphasia (fluent speech with poor comprehension). 
- **Dual-Stream Model:** 
 - **Ventral (“what”)**: maps sounds → meanings (recognition, comprehension). 
 - **Dorsal (“how”)**: maps sounds → articulatory representations (imitation, repetition).

---

## 📝 Pre-Class Activities
1. **Read** pp. 51–71. Highlight: coarticulation examples, categorical perception graphs, McGurk explanation, theory summaries. 
2. **Quick listen:** Find two recordings of the **same word** by different speakers (YouTube or phone recordings). Note acoustic differences you notice. 
3. **Optional (2–3 min video):** Watch a short **McGurk effect** clip to prime your intuition.

---

## 💬 In-Class Activities

### 1) Categorical Perception Demo (10 min)
- Listen to a **ba–pa VOT continuum** (instructor-provided audio). 
- Individually mark where your **category boundary** falls; compare with neighbors. 
- Discuss: Why do we perceive a **sharp switch** despite gradual acoustic change?

### 2) Coarticulation Lab (12 min)
- Pair up and record each other saying **“key”** vs **“coo”**, **“boot”** vs **“beat.”** 
- Listen back: describe differences in the **/k/** burst quality and formant transitions; relate to **anticipatory** coarticulation. 
- Share one observation with the class.

### 3) McGurk + Lipreading Challenge (15 min)
- Watch a **McGurk effect** video (with/without sound). 
- **Lipreading mini-task:** Instructor silently mouths a short phrase; students write what they think they “heard.” 
- Discuss: When did visual cues help or hurt? Connect to **noisy environments** and **L2 listening**.

### 4) Theory Triads (15 min)
- Groups of three: each person briefly **champions** one theory (**Motor**, **Auditory**, **TRACE**). 
- Apply your theory to explain: **(a)** McGurk effect, **(b)** categorical perception, **(c)** Ganong effect (bias toward real words). 
- As a group, write one **prediction** your theory makes about perception **in noise** or **with strong context**.

### 5) Noisy Room Challenge (10 min)
- One student reads a sentence list while teammates create **background noise** (whispered chatter or soft music). 
- Listeners transcribe best guesses; repeat while **watching the talker’s face**. 
- Compare accuracy: audio-only vs audiovisual. What improved and why?

### 6) Quick Wrap (3 min)
- On a sticky note: write **one insight** about **audiovisual integration** and **one question** to revisit.

---

## 🔁 Post-Class Review
- **Sketch** a one-page comparison: **Motor vs Auditory vs TRACE** (key idea, 1 strength, 1 limitation, 1 prediction). 
- **Reflect (100–120 words):** When do **visual cues** help you most in English listening? Any times they mislead? 
- **Bonus:** Try the **Laurel/Yanny** clip and explain (briefly) why people disagree.

---

## 🏠 Homework
- **Textbook “Test Yourself”** items on **coarticulation**, **categorical perception**, and **McGurk** (from pp. 51–71 section). 
- **Short write-up (≈150 words):** Based on the Noisy Room Challenge, argue **which theory** best explains your results and **why**. Use one concrete observation.

---

## 🧩 Self-Check Questions

**Q1.** What’s the **lack of invariance** problem, and how do humans cope with it? -->
&lt;!--*A:* The same phoneme has many acoustic realizations. Perceivers use **context**, **categories**, and **top-down knowledge** to map variable signals to stable phonemes.*-->
&lt;!--
**Q2.** Define **coarticulation** and give one concrete example. -->
&lt;!--*A:* Overlap of articulatory gestures; e.g., /k/ is fronted in **key** and backed in **coo**, changing acoustic cues.*-->
&lt;!--
**Q3.** What pattern shows **categorical perception** in a VOT continuum? -->
&lt;!--*A:* A **sharp identification boundary** and **better discrimination across** the boundary than within a category.*-->
&lt;!--
**Q4.** How would **Motor Theory** explain the McGurk effect? -->
&lt;!--*A:* Perception uses **articulatory gestures**; conflicting visual gestures bias the perceived sound.*-->
&lt;!--
**Q5.** What does **TRACE** predict when context strongly supports a real word? -->
&lt;!--*A:* **Top-down activation** from the lexical layer biases phoneme perception (e.g., **Ganong effect**).-->
&lt;hr>
&lt;!--
## 🧰 Key Terms
**Coarticulation**, **Categorical perception**, **Voice Onset Time (VOT)**, **McGurk effect**, **Lexical bias / Ganong effect**, **Motor Theory**, **Auditory Theory**, **TRACE model**, **Superior temporal gyrus (STG)**, **Wernicke’s area**, **Dual-stream (ventral/dorsal)**.

---

## 🌐 Optional Resources
- Short demos: **VOT continua**, **McGurk effect** (search on YouTube or psycholinguistics demo sites). 
- Popular explainers on **Laurel vs. Yanny** and audiovisual speech. 
- Intro readings on the **dual-stream model** of speech processing.

---

### ✅ How to use these notes
- **Before class:** read, preview key terms, try noticing coarticulation in everyday speech. 
- **During class:** participate in demos, take quick notes on what **helped** vs **hurt** perception. 
- **After class:** complete comparison chart and write-up; revisit McGurk with friends/family and see who “hears” what.

-->
&lt;!--
## 📘 Overview

How do we recognize a word—faster than the blink of an eye—and retrieve its meaning from our mental lexicon? This week, we explore how the brain matches sounds or letters to stored words, and what factors influence how fast and accurately we do this. We’ll also examine classic experiments like **lexical decision tasks** and **semantic priming**, and learn how computational models try to explain how words are accessed in real time.

---

## 🎯 Learning Goals

By the end of this week, you should be able to:

- Explain the concept of **lexical access** and how words are retrieved from memory.
- Describe how **lexical decision** and **semantic priming** tasks reveal patterns in word processing.
- Understand how factors like **word frequency**, **neighborhood density**, and **context** influence word recognition.
- Compare key **models of lexical access**, including **logogen**, **cohort**, and **interactive activation** models.

---

## 📖 Required Reading

- **Chapter 3 (pp. 79–113)** from *Introduction to Psycholinguistics: Understanding Language Science* by Matthew Traxler.

---

## 🧠 Core Concepts

### 📚 What Is Lexical Access?

- **Lexical access** refers to the process of recognizing a word and retrieving its meaning and sound from the mental lexicon.
- This involves:
 - **Activation** of candidate words.
 - **Selection** of the correct word from similar alternatives.
 - **Integration** with the sentence or context.

---

### ⚡ Lexical Decision Task

- Participants decide if a string of letters is a real word or not (e.g., *table* vs. *blafe*).
- **Key finding**: Response times are faster for:
 - **High-frequency words** than low-frequency ones.
 - **Words preceded by related words** (e.g., *doctor → nurse*).

**Example**: 
You’ll respond faster to *“bread”* if you just saw *“butter.”*

---

### 🧠 Semantic Priming

- When a word is processed more quickly because it's related in meaning to a previous word.
- Shows how **concepts are interconnected** in the mental lexicon.

**Classic study**: Meyer &amp; Schvaneveldt (1971) 
- Faster responses for semantically related word pairs (*nurse-doctor*) than unrelated ones (*nurse-butter*).

---

### 🔠 Word Frequency &amp; Neighborhood Effects

- **Frequency effect**: Common words like *book* are accessed faster than rare words like *niche*.
- **Orthographic neighbors**: Words that differ by one letter (e.g., *book*, *look*, *cook*).
 - **Dense neighborhoods** can speed up or slow down access depending on context.
- **Phonological neighbors**: Sound-based overlap (e.g., *cat*, *cap*, *cab*).

---

### 🧬 Models of Word Recognition

| Model | Key Features |
|------------------------|----------------------------------------------------------------------|
| **Logogen Model** | Each word has a threshold-based unit; frequency lowers the threshold. |
| **Cohort Model** | Word recognition begins from the first sound and narrows down options. |
| **Interactive Activation Model** | Uses letter and word layers with top-down and bottom-up activation. |

---

## 📝 Pre-Class Activities

Please complete these before class:

1. 📖 **Read pp. 79–113** in Chapter 3, focusing on the experiments and models described.
2. 💡 **Reflection Prompt**:
 > Think of a time when you misheard or misread a word. What caused the confusion—sound, context, spelling?
3. 👀 Watch this 2-min intro: [Lexical Decision Task Explained](https://www.youtube.com/watch?v=WqjqfE7xGvA)
4. 📄 Optional read: 
 - [Semantic Priming (Very Brief Overview)](https://psychology.iresearchnet.com/cognitive-psychology/memory/semantic-priming/)

---

## 💬 In-Class Activities

- 🧪 **Lexical Decision Demo**: Try judging real vs. non-words under time pressure.
- 🧠 **Semantic Priming Experiment**: Predict which pairs will speed recognition.
- 🔄 **Word Frequency Sorting**: Rank a list of words by expected recognition speed.
- 🧩 **Model Comparison Task**: Match findings (e.g., priming, frequency) to the model that explains them best.

---

## 🔁 Post-Class Review

After class:

1. ✍️ Answer these questions in your notes:
 - What is the evidence that words are organized by meaning?
 - Why are high-frequency words recognized faster?
2. 🔍 Try this experiment online: 
 - [Semantic Priming Test](https://faculty.washington.edu/chudler/java/priming.html)
3. 💬 Join the class forum: 
 > Share a pair of semantically related words in your language and explain why you think they're closely linked.

---

## 🏠 Homework

- 📖 Revisit the section on **lexical decision tasks** and **semantic priming**.
- ✍️ Complete at least two *Test Yourself* questions from this chapter.
- 🧠 Mini Assignment:
 > Choose five words and identify their semantic neighbors. Write a short paragraph explaining the associations.

---

## 🔜 Coming Up Next

Next week, we dive deeper into **how we understand word meanings** and the role of **context** and **ambiguity** in the brain’s interpretation process.

-->
&lt;!--
## 📘 Overview

This week focuses on how words are **represented**, **stored**, and **retrieved** in the mind. We examine various models of the **mental lexicon**, how meaning is structured, and how the mind retrieves lexical items efficiently. You’ll learn about traditional **associationist theories**, the **symbol grounding problem**, and **embodied semantics**, as well as first- to third-generation computational models.

---

## 🧠 Core Concepts

### What Is a Word?

- Psycholinguists distinguish between:
 - **Lexeme**: the form of the word (e.g., “run,” “ran,” “runs”)
 - **Lemma**: an abstract entry in the lexicon with semantic/syntactic info
- Words have **semantic**, **syntactic**, and **phonological** properties that are processed in different stages.

---

### Lexical Semantics

- Focuses on how word meanings are **represented and structured** in memory.
- Words are **linked by meaning, context, and usage** (e.g., “dog” → “bark”, “bone”).

---

### Associationist Models

- **HAL (Hyperspace Analogue to Language)** and **LSA (Latent Semantic Analysis)** represent meaning based on **co-occurrence** in language.
- Meaning is derived from **distributional patterns** across contexts:contentReference[oaicite:0]{index=0}.
- These models help explain **priming effects**, **semantic distance**, and **category structure**.

---

### The Symbol Grounding Problem

- How do symbols (e.g., words) connect to real-world experiences?
- Purely symbolic models (like HAL/LSA) struggle to explain **how symbols acquire meaning**.

---

### Embodied Semantics

- Suggests meanings are grounded in **sensorimotor experiences**.
- E.g., understanding “kick” involves partial activation of brain areas associated with **motor planning for the legs**.
- Supported by neuroimaging studies showing **motor cortex involvement** in language comprehension:contentReference[oaicite:1]{index=1}.

---

### Lexical Access Models

| Generation | Features |
|------------|----------|
| **First** | Serial search models: words are retrieved by scanning a list of candidates (like dictionary lookup) |
| **Second** | Frequency-based access: high-frequency words are accessed faster (logogen model) |
| **Third** | Distributed models: words are activated in **parallel**, with activation levels depending on frequency and context |

- Third-generation models combine features from earlier approaches and handle **ambiguity, priming, and prediction** better:contentReference[oaicite:2]{index=2}.

---

## 📚 Reading

- Traxler (2012), Chapter 3: *Word Processing* (pp. 79–113)

---

## 🏷️ Key Terms

| Term | Definition |
|------|------------|
| **Lexeme** | The morphological/phonological form of a word |
| **Lemma** | A mental representation of a word's syntactic and semantic information |
| **Associationist Model** | Theories that represent meaning via word co-occurrence and proximity in vector space |
| **Symbol Grounding Problem** | The challenge of linking abstract symbols to real-world referents |
| **Embodied Semantics** | The view that meaning is grounded in sensory and motor systems |
| **Lexical Access** | The process of retrieving a word from memory during comprehension or production |

---

## 🧪 Examples &amp; In-Class Activities

### 🧠 Word Web Brainstorm

- Given a target word (e.g., “apple”), students quickly generate related words.
- Use to demonstrate **semantic networks** and **priming**.

### 🖼️ HAL vs. Embodiment Demo

- Students sort words by co-occurrence (text-based) vs. experience (sensorimotor).
- Discussion: what’s missing from co-occurrence models?

### 💬 Priming Reaction Task

- Present pairs like “doctor–nurse” and “bread–nurse.”
- Measure how semantic relatedness speeds up response time.

### 🧪 Lexical Access Simulation

- Simulate lexical activation using cards or diagrams:
 - “cat” activates “dog,” “fur,” “meow,” etc.
 - Show how **frequency**, **context**, and **ambiguity** shape activation.

---

## ❓ Self-Check Questions

1. What is the difference between a lemma and a lexeme?
2. How do HAL and LSA models explain word meaning?
3. What is the symbol grounding problem, and how do embodied accounts address it?
4. Compare first-, second-, and third-generation models of lexical access.
5. How does context influence which meaning of an ambiguous word is accessed?

---

## 🧩 Practice Prompt (Adapted from the Textbook)

> Design a small-scale experiment to test the effects of word frequency on lexical access speed. 
> - What method would you use? 
> - What results would support a frequency-based model? 
> - How might priming or context alter your findings?

---

## 🔁 Related Chapters

- Chapter 2: *Speech Production and Comprehension*
- Chapter 4: *Sentence Processing* (syntactic context and meaning selection)


&lt;!--
## 🧠 Chapter 4 Lecture Notes: Language Acquisition II

In this chapter, we explore the development of more complex linguistic structures in children’s speech — especially grammar, morphology, and syntax. We also examine what overregularization errors and bilingual development can reveal about the nature of language learning.

---

## 📘 Core Topics &amp; Concepts

### 1. Grammatical Development

* Children move from two-word combinations to more structured utterances
* **Syntactic bootstrapping**: using sentence structure to infer word meaning
* Early word order patterns show awareness of grammar (e.g., “Mommy sock” = “Mommy has a sock” or “Put the sock on Mommy”)

> 📈 Children’s utterances gradually become more adult-like but reflect creative rule-building

> 🧠 **In class**: We’ll trace the growth of sentence length and complexity using transcript examples.

---

### 2. Morphological Development

#### Inflectional Morphology

* Regular past tense: “walked,” “played”
* Irregular forms: “went,” “broke”

> 🧩 **Overregularization**:
>
> * Children apply regular rules to irregular verbs (e.g., “goed,” “runned”)
> * Evidence that children internalize rules, not just imitate

#### Derivational Morphology

* Forming new words from existing ones (e.g., “teacher” from “teach”)
* Acquired later than inflectional morphology

> 🎧 **In class**: We’ll listen to recordings of overregularization and discuss what they reveal about internal rule systems.

---

### 3. Negative and Question Formation

* Children begin by placing negation at sentence edges (e.g., “No go outside”)
* Question development:

 * Stage 1: Rising intonation only (“You want juice?”)
 * Stage 2: Use of auxiliaries and inversion (“Can I go?”)

> 🧠 Errors like “Why she can’t go?” are systematic and reflect transitional grammar stages.

---

### 4. Bilingual Language Acquisition

* Simultaneous bilingualism: acquiring two languages from birth
* Sequential bilingualism: learning a second language after establishing the first
* Children often code-switch — combine elements from both languages — but do so systematically

> 🔍 **Research Spotlight**: Bilingual infants can separate languages by rhythm and intonation before they can speak.

> 🧪 **In class**: We’ll analyze bilingual transcripts and examine code-switching patterns.

---

### 5. Input, Feedback, and the Role of Caregivers

* Caregivers rarely correct grammar directly — but they provide feedback through recasts (reformulations)
* **Implicit correction**: “Her goed to the store” → “Yes, she went to the store.”
* Quantity and quality of input shape grammatical development

---

## 🔁 Summary Table

| Concept | Description | Example |
| ----------------------- | ----------------------------------------- | ----------------------------------- |
| Overregularization | Applying regular rules to irregular forms | “goed,” “runned” |
| Syntactic Bootstrapping | Using grammar to infer meaning | “The dax is crying” → dax = animate |
| Derivational Morphology | Making new words with suffixes/prefixes | "runner,” "happiness” |
| Code-Switching | Mixing languages in systematic ways | “I want leche.” |
| Recasts | Caregiver corrections through repetition | “She go?” → “Yes, she goes.” |

---

## 📝 Self-Review Questions

1. What is overregularization, and what does it tell us about grammar learning?
2. How does question formation develop over time?
3. What distinguishes inflectional from derivational morphology?
4. What are the characteristics of bilingual language acquisition?
5. How do caregivers support grammatical development?

---

## 📂 In-Class Resources and References

* 🎧 *Child Language Recordings*: Overregularization and early questions
* 📝 *Bilingual Transcript Samples*: Code-switching in young children
* 📄 *Developmental Chart*: Stages of question and negation formation
* 📚 *Optional Reading*: Genesee et al. (2004), “Dual Language Development &amp; Disorders”

---

> 📖 Reading: Chapter 4, pp. 64–89 from *Introduction to Psycholinguistics* by Traxler
--></description></item><item><title>Week 3: 🔊 Speech Planning &amp; Errors II</title><link>https://zjpsycholin.github.io/psycholinguistics/week03/</link><pubDate>Sun, 04 May 2025 10:03:44 -0400</pubDate><guid>https://zjpsycholin.github.io/psycholinguistics/week03/</guid><description>&lt;h2 id="-overview">📘 Overview&lt;/h2>
&lt;p>This week we extend Week 2’s production pipeline by digging deeper into &lt;strong>what errors tell us about planning&lt;/strong>, how speakers &lt;strong>monitor and repair&lt;/strong> speech, and how &lt;strong>articulation&lt;/strong> shapes the final output. You’ll run an &lt;strong>error diary mini-study&lt;/strong>, practice &lt;strong>self-monitoring&lt;/strong> on short speaking tasks, and try &lt;strong>articulation drills&lt;/strong> (minimal pairs &amp;amp; tongue twisters) to feel planning pressure and coarticulation effects.&lt;/p>
&lt;hr>
&lt;h2 id="-learning-goals">🎯 Learning Goals&lt;/h2>
&lt;p>By the end of Week 3, you should be able to:&lt;/p></description></item><item><title>Week 2: Speech Planning &amp; Errors I</title><link>https://zjpsycholin.github.io/psycholinguistics/week02/</link><pubDate>Wed, 30 Apr 2025 10:03:44 -0400</pubDate><guid>https://zjpsycholin.github.io/psycholinguistics/week02/</guid><description>&lt;h2 id="-overview">📘 Overview&lt;/h2>
&lt;p>This week we look inside the moment-to-moment planning that turns thoughts into speech.&lt;br>
You’ll meet a standard &lt;strong>speech production architecture&lt;/strong> (concept → words → sounds → articulation), see why &lt;strong>errors&lt;/strong> are scientifically useful, and experience classic &lt;strong>naming tasks&lt;/strong> that reveal the timing of semantic and phonological processes.&lt;br>
We’ll also discuss &lt;strong>Tip-of-the-Tongue (TOT)&lt;/strong> states and what they show about the mental lexicon.&lt;/p>
&lt;hr>
&lt;h2 id="-learning-goals">🎯 Learning Goals&lt;/h2>
&lt;p>By the end of Week 2, you should be able to:&lt;/p></description></item><item><title>🧠 Week 1: What is Psycholinguistics?</title><link>https://zjpsycholin.github.io/psycholinguistics/week01/</link><pubDate>Fri, 25 Apr 2025 10:03:44 -0400</pubDate><guid>https://zjpsycholin.github.io/psycholinguistics/week01/</guid><description>&lt;h2 id="-overview">📘 Overview&lt;/h2>
&lt;p>Welcome to &lt;em>Introduction to Psycholinguistics&lt;/em>! This week, we begin by exploring what psycholinguistics is, what questions it asks, and how it connects to other fields. You’ll learn about what makes human language unique and how language relates to cognition, evolution, and communication.&lt;/p>
&lt;p>We’ll also discuss myths about language, compare human and non-human communication systems, and learn how language may (or may not) shape the way we think.&lt;/p>
&lt;hr>
&lt;h2 id="-learning-goals">🎯 Learning Goals&lt;/h2>
&lt;p>By the end of Week 1, you should be able to:&lt;/p></description></item><item><title>Syllabus</title><link>https://zjpsycholin.github.io/psycholinguistics/syllabus/</link><pubDate>Fri, 18 Apr 2025 10:03:44 -0400</pubDate><guid>https://zjpsycholin.github.io/psycholinguistics/syllabus/</guid><description>&lt;h1 id="-course-syllabus-introduction-to-psycholinguistics">📘 Course Syllabus: &lt;em>Introduction to Psycholinguistics&lt;/em>&lt;/h1>
&lt;p>&lt;strong>Instructor:&lt;/strong> Zhang Jun&lt;br>
&lt;strong>Class Schedule:&lt;/strong> Two 45-minute sessions per week&lt;br>
&lt;strong>Email:&lt;/strong> &lt;a href="mailto:jzhang3@ahu.edu.cn">jzhang3@ahu.edu.cn&lt;/a>&lt;/p>
&lt;hr>
&lt;h2 id="-course-introduction">🧠 Course Introduction&lt;/h2>
&lt;p>&lt;strong>Psycholinguistics&lt;/strong> is the scientific study of how humans acquire, produce, understand, and represent language in the mind. This course introduces foundational theories, research methods, and key findings in the field, offering an integrated view of linguistic knowledge, language development, processing mechanisms, and neural underpinnings. Topics include speech perception, word recognition, sentence parsing, discourse comprehension, bilingualism, non-literal language, and sign language.&lt;/p></description></item></channel></rss>